
The more I work on machine learning, the more I'm embarrased to admit that most of what I'm worked on is all so blindingly simple.  At the core of essentially all of modern machine learning is a single
objective: KL minimization, and there is a very simple recipe to follow to rederive
most of the named objectives out there. 
In the past I've discussed some of the 
<a href="kl.html">special properties of KL divergence</a>, or how you can derive
<a href="diffusion.html">VAEs or Diffusion Models</a> by means of a simple KL objective, but in 
light of a recent <a href="https://www.alexalemi.com/talks/information-theory-for-representation-learning.html">talk</a> I gave at the InfoCog Workshop at NeurIPS 2024, I'd like to prepare essentially a written copy of that talk.<a href="#multivariate-ib"><sup>xxa-attribution</sup></a>

<aside> <sup id="figattribution">xxa-attribution</sup>
This is also essentially my own retelling of <a href="https://www.cs.huji.ac.il/labs/learning/Theses/Slonim_PhD.pdf">Noam Slonim's thesis on the Multivariate Information Bottlneck</a>.
</aside>


<figure id="#conditional" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/kl-elephant.png"
    alt="A cartoon depicting several blindfolded scientists analyzing different parts of an elephant, the different scientists think they are looking at VIB, or Diffusion or BNNs or VAEs or Semi-supervised Learning or Bayesian inference, but really its all just KL Divergence.">
  <figcaption>
  Figure xxf1. The elephant in the room is KL divergence or the relevant entropy.<a href="#figattribution"><sup>xxa2</sup></a>

  </figcaption>
  </center>
</figure> 

<aside> <sup id="figattribution">xxa2</sup>
Cartoon modified from Kevan C. Herold, Jeffrey A. Bluestone ,Type 1 Diabetes Immunotherapy: Is the Glass Half Empty or Half Full?. Sci. Transl. Med.3,95fs1-95fs1(2011).
<a href="https://doi.org/10.1126/scitranslmed.3002981">DOI:10.1126/scitranslmed.3002981</a>.
</aside>


## KL Divergence as Expected Weight of Evidence

Before we get into it, we need to make sure we're all starting on the same page.  I've written <a href="kl.html">before</a> about part of what makes KL divergence or the relative entropy so special, but for the purposes of the journey we are about to undertake, let's use the interpretation of KL divergence as <a href="kl.html#expected-weight-of-evidence">an expected weight of evidence</a>, which I'll briefly repeat here.

Imagine we have two hypotheses $P$ and $Q$ and we're trying to decide which of these two models is a better model of the world.  We go out an collect some data $D$ and would like to use that data to help us decide between the two models.  Being good probabilistic thinkers with a penchant for gambling, what we're interested in is:

$$ \frac{\Pr(P|D)}{\Pr(Q|D)}, $$

the <a href="https://en.wikipedia.org/wiki/Odds"><i>odds</i></a> of $P$ versus $Q$. Using <a href="https://en.wikipedia.org/wiki/Bayes\%27_theorem">Bayes rule</a> we can express this as:

$$ \frac{\Pr(P|D)}{\Pr(Q|D)} = \frac{\Pr(D|P)}{\Pr(D|Q)} \frac{\Pr(P)}{\Pr(Q)}, $$

the product of the <a href="https://en.wikipedia.org/wiki/Likelihood_function"><i>likelihood</i></a> ratio that the data we observed were generated by model $P$ and $Q$ times the <i>prior odds</i> of the two models.  Taking a logarithm of both sides turns the product into an easier to work with sum:

$$ \log \frac{\Pr(P|D)}{\Pr(Q|D)} = \log \frac{\Pr(D|P)}{\Pr(D|Q)} + \log \frac{\Pr(P)}{\Pr(Q)}. $$

Now, the <i>posterior log odds</i> is expressed as the sum of the <i>weight of evidence</i> plus the <i>prior log odds</i> of the two hypotheses.

<figure id="#belief-of-meter class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/belief-o-meter.png"
    alt="A cartoon representation of a Belief-O-Meter as an old school linear analog meter.">
  <figcaption>
  Figure xxf0. Belief-O-Meter.
  </figcaption>
  </center>
</figure> 

This *weight of evidence* tells us how much to update our beliefs in light of evidence.  If you picture a sort of Belief-O-Meterâ„¢ for your own beliefs, each bit of independent evidence gives you an additive update for the meter, pushing your beliefs either left or right, towards either $P$ or $Q$.  For simple hypothesis taking the form of probability distributions, this weight of evidence is just the log density ratios of the data under the models:

$$ \log \frac{\Pr(D|P)}{\Pr(D|Q)} \to \log \frac{p(D)}{q(D)}. $$

What then is <a href="https://en.wikipedia.org/wiki/Kullback\%E2\%80\%93Leibler_divergence">the Kullback-Leibler (KL) divergence</a>? Imagine if one of our two hypothesis is actually true.  If $P$ was the probability distribution governing the actual world, the <i>expected weight of evidence</i> we would accumulate from observing some data would be:<a href="#brakets"><sup>xxa3></sup></a>

$$ I[p;q] \equiv \int dx\, p(x) \log \frac{p(x)}{q(x)} \equiv \left\langle \log \frac{p(x)}{q(x)} \right\rangle_{p(x)} . $$
<aside> <sup id="brakets">xxa3</sup>
To clean up the notation, I like using brakets: $ \langle \cdot \rangle_p \equiv \mathbb{E}_{p}[\cdot] \equiv \int dx\, p(x) [\cdot]$, and to clean things up further (or because I'm lazy) I'll often leave off the substrict saying which distribution the brakets are to be taken with respect to, in any of those cases you can assume its a full joint $p$ distribution (over any variables that are otherwise unbound).
</aside>

We can interpret the KL divergence as a measure of how quickly we would be able to discern between hypotheses $P$ and $Q$ if $P$ were true.  Similarly, the <i>reverse KL</i> is:

$$ I[q;p] \equiv \int dx\, q(x) \log \frac{p(x)}{q(x)} \equiv \left\langle \log \frac{p(x)}{q(x)} \right\rangle_{q(x)}, $$


a measure of how quickly we'd be able to discern between $P$ And $Q$ if $Q$ were true.  Suddenly, the asymmetry of the KL divergence, an issue that often causes costernation is no longer a mystery.  We should expect the expected weight of evidence to be asymmetric.   As an extreme example, imagine we were trying to decide between two hypothesis regarding some coin flips we are about to observe.  $P$ is the hypothesis that the coin is fair while $Q$ is the hypothesis the coin is a cheat, double-headed coin.  In this case, if we actually had a fair coin, we expect to be able to perfectly discern the two hypotheses (infinite KL) because we will eventually observe a tails, an impossible situation under the alternative ($Q$) hypothesis.  Meanwhile, if the coin is actually a cheat, we'll be able to collect, on average, 1 bit of evidence per flip in favor of the hypothesis that the coin is a cheat, but we will only ever observe heads and so never be able to perfectly rule out the possibility that the coin is fair and we've simply observed some miracle.<a href="#million"><sup>xxa4</sup></a>

<aside> <sup id="million">xxa4</sup>
As a true aside, people often say "one in a million" but lack a good mental model of just how rare that is.  20 heads in a row for a fair coin is a one in a million event. This fun fact and others can be found <a href="https://www.stat.berkeley.edu/~aldous/Real-World/million.html">here, at David Aldous's Home Page</a>.
</aside>

## Mathematical Properties

In what follows, we'll need to use two mathematical properties of the KL divergence. The first is that the KL divergence is non-negative, i.e. the lowest it can be is zero:

$$ I[p;q] \equiv \int dx\, p(x) \log \frac{p(x)}{q(x)} \geq 0, $$ 

which I'll leave as an exercise to the reader, or you can see a proof in the <a href="kl.html#non-negative-proof">previous post</a>. In the context of our interpretation of KL divergence as an expected weight of evidence, the non-negativity of KL divergence means, essentially, that the world can't lie to us.  If we are trying to decide between two hypotheses, and one of them happens to be correct, we have to, we must, we have to, we must, on average, be pushed in the direction of the correct hypothesis.

The other property we'll use is the *monotonicity* of the KL divergence.  This is a generalized version of the [data processing inequality](https://en.wikipedia.org/wiki/Data_processing_inequality).  If we perform some kind of processing on our random variables, it should only make it harder to discern between two hypotheses, not easier.  In particular, the version we'll need today concerns *marginalization*, if I have two joint distributions defined on two random variables, it always has to be the case that the KL divergence between their two marginals must be less than or equal to the joint KL:
$$ \int dx\, dy\, p(x,y) \log \frac{p(x,y)}{q(x,y)} \geq \int dx\, p(x) \log \frac{p(x)}{q(x)}, $$
which is easy to show if you decompose $p(x,y) = p(x) p(y|x)$ and use the fact that all KL divergences (including the conditional $I[p(y|x);q(y|x)] \geq 0$ are non-negative.

Again, in terms of our current interpretation, this makes sense, if I have some beliefs defined over several variables, if I only get to observe some subset of them, it should be harder for me to discern the beliefs.  The less I look at, the less I see.

## Universal Recipe

With the prerequisites out of they way we're reading to see the "universal recipe" for generating objectives. Regardless of what we are trying to accomplish, the goal is to make the real world look more like our dreams.  Given that KL diverence is the *proper* way to measure how similar two distributions are, we need only minimize the KL divergence between the real world, the world we can sample from and the world as we wish it were.  The smaller that KL can become, the harder it becomes for us or anyone else to distinguish between our dreams and reality.  In steps:

 1. Draw a causal graphical model corresponding to the world as it is, the true world $P$.
 2. Augment the real world with any components you wish to add.
 3. Draw the world of your desires, what success would look like, what you are targetting, the dream world $Q$.
 4. Minimize $I[P;Q]$.
 5. ...
 6. Profit!

As simple as it sounds, in retrospect I think a lot of my previous papers were just following this recipe.  Let's repeat this ad nauseum.

## Density Estimation

We'll start with the problem of density estimation.  Let's say we have some black box that generates samples.  This is the real world $P$, outside of our control.  Despite not knowing how $p(x)$ is structured, we can push the button the black box to generate samples.  What do we wish for? We wish we instead had a nice description of those same images.  We wish that those images instead came from a box of our own design, some parametric model or probability distribution with knobs that we can adjust to bring it into alignment with the real world, our dream world $q_\theta(x)$ with parameters $\theta$.

<figure id="#density-estimation" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/density-estimation.png"
    alt="A graphical version of density estimation, the left shows the distribution $p(x)$ where $X$ is a random variable denoting images. The right shows the same, but labeled as $q_\theta(x)$.">
  <figcaption>
  Figure xxf2. Density Estimation.
  </figcaption>
  </center>
</figure> 

Following the recipe, our recipe then is to minimizing the KL divergence between the real world:

$$ I[p; q] = \left\langle \log \frac{p(x)}{q_\theta(x)} \right\rangle_p $$

To belabor the point, in terms of our interpretation of KL divergence, this makes sense. $I[p;q]$ measures how easy it is for us to distinguish between $p$ and $q$ using samples from $p$. We have samples from $p$, while $q_\theta(x)$ is a whole set of worlds we can index with our parameters $\theta$.  We seek a setting of those parameters which make it as difficult as possible for us or anyone else to tell the difference between the real world $P$ and our imaginary one $Q$.  Minimizing the KL divergence does exactly that.  

Unfortunately, naively, this objective requires that we be able to evaluate $\log p(x)$, the density the real world assigns to the samples it generates.  This is out of reach, we don't know what the real world is doing, but here is where the KL divergece helps us out yet again.  It decomposes into two terms:

$$ \underbrace{\left\langle \log \frac{p(x)}{q_Î¸(x)} \right\rangle}_{I[p;q]} = \underbrace{\left\langle \log p(x) \right\rangle \vphantom{\left\langle \frac p q \right\rangle} }_{-H[p]} - \underbrace{\left\langle \log q_Î¸(x) \right\rangle \vphantom{\left\langle \frac p q \right\rangle}}_{H[p;q]} $$

The (negative) *entropy* of the true data generating process ($H[p]$), and the *cross-entropy* between $p$ and $q$: ($H[p;q]$), aka the *likelihood* of the data samples from $p$ under $q$.  The entropy of the true data generating process isn't something that we control, as far as we're concerned its a constant and we don't need to worry about it as far as our objective is concerned. Just like that, we see that minimizing the KL divergence between the real world and the world of our desires, in this simple single random variable setup recovers ordinary minimum [cross-entropy](https://en.wikipedia.org/wiki/Cross-entropy) learning, aka maximum likelihood learning, but with a different and hopefully well motivated origin.  We adjust the parameters of our model $q_\theta(x)$ so as to maximize the likelihood of the data $\log q_\theta(x)$, why? So that we and anyone else would struggle as much as possible to distinguish between the real world and our model.  Once we identify this motivation, lots of other machine learning objectives will fall into place as being versions of the same thing.

There is one caveat, I'm [a particular stickler](kl.html#appendix-a) for decomposing KL divergences in this way. I don't think it makes any dimensional sense.  I can't take the logarithm of a dimensional quantity, let alone a density, so I'm going to ruin the flow of this post to address this aside.  To fix the glitch, let's instead try to explicitly choose some base measure $m(x)$ for which we do know the densities and insert it into our original objective:

$$ \left\langle \log \frac{p(x)}{q_\theta(x)} \right\rangle_p = \left\langle \log \frac{p(x) m(x)}{q_\theta(x)m(x)} \right\rangle = \left\langle \log \frac{p(x)}{m(x)} \right\rangle_p - \left\langle \log \frac{m(x)}{q_\theta(x)} \right\rangle_p $$ 

Now, we've decomposed the KL divergence between $P$ and $Q$ into two terms, the first is the KL divergence between $P$ and $M$, our base density. Just as before, this is some constant outside our control, as long as we fix $m(x)$, given that $p(x)$ is fixed, their KL divergence is fixed and no changes we make to $\theta$ have any effect, so we can drop this (now appropriately reparameterization-independent) term from our objective.  We're left with the weight of evidence samples from $p$ provide in favor of $m$ against $q$. If we try to adjust the parameters of $q_\theta(x)$ to make it easiest as possible to distinguish from some base measure $m(x)$ as possible under samples from $p$ we and ensure that we actually drive $q$ towards $p$.  If we use ordinarly path gradients the choice of $m(x)$ here won't actually effect the gradients. It will however help us sleep at night, ensuring that our objective is a truly reparameterization-invariant quantity.<a href="#control-variates'><sup>xxa4</sup></a>

<aside> <sup id="control-variates">xxa4</sup>
If I'm being honest, this is something that bothers me that I don't fully understand.  Using a baseline model here functionally takes the same form as control variates like the baselines used in REINFORCE, but here they don't help at all (we aren't taking an expectation with respect to $q$ here). Regardless, it really feels like an appropriate base measure <i>ought</i> to help.  I can't help but think that it signals a problem with the gradients we take in machine learning today. Things like <a href="https://arxiv.org/abs/2206.07137">RHO-Loss</a> reinforce this idea.
</aside>

TODO Empirical distribution and finite samples.

## Supervised Learning

Let's complicate things slightly.  Instead of imagining that we have a single random variable in the real world, imagine instead we have a pair of variables, $X$ and $Y$.  For concreteness, imagine the $X$ are images and the $Y$ are their associated labels in some dataset. 

What are we after? What does success look like? Let's imagine that what we desire is the ability to assign labels to data.  What we wish were the case was that we used the same process to draw the images $q(x) = p(x)$, but instead of using the real world process to assign labels, ideally the labels would instead come from a device under our control: $q_\theta(y|x)$. <a href="#theta"><sup>xxa5</sup></a>.  Just as before, we simply minimize the KL divergence between these two joints and we obtain an objective:


<figure id="#supervised-learning" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/supervised-learning.png"
    alt="A graphical version of supervised learning, the left shows the distribution $p(y,x)$ where $X$ is a random variable denoting images and $Y$ are some labels. The right shows the world of our dreams, where we draw the images $X$ from the same process as in the real world $p(x)$ but we have a machine that applies the labels: $q(y|x)$.">
  <figcaption>
  Figure xxf3. Supervised Learning.
  </figcaption>
  </center>
</figure> 

<aside> <sup id="theta">xxa5</sup>
I'm going to start dropping the subscript $\theta$ for the parameters.
</aside>

$$ \left\langle \log \frac{p(x,y)}{p(x)q(y|x)} \right\rangle, $$

Just as above, when we drop constants outside of our control, we end up with the usual maximum likelihood objective we are used to.


## Variational Autoencoders


<figure id="vae" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/vae.png"
    alt="A graphical version of a VAE.">
  <figcaption>
  Figure xxf4. Variational Autoencoders.
  </figcaption>
  </center>
</figure> 


## Variational Information Bottleneck

<figure id="vib" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/vib.png"
    alt="A graphical version of VIB.">
  <figcaption>
  Figure xxf5. Variational Information Bottleneck.
  </figcaption>
  </center>
</figure> 

## Diffusion

<figure id="diffusion" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/diffusion.png"
    alt="A graphical version of Diffusion.">
  <figcaption>
  Figure xxf6. Variational Diffusion.
  </figcaption>
  </center>
</figure> 

## Semi-Supervised Learning

<figure id="semi-supervised" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/semi-supervised.png"
    alt="A graphical version of a Semi-supervised VAE.">
  <figcaption>
  Figure xxf7. Semi-Supervised Variational Autoencoder.
  </figcaption>
  </center>
</figure> 

## Bayesian Inference

<figure id="bayes" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/bayes.png"
    alt="A graphical version of Bayesian Inference.">
  <figcaption>
  Figure xxf8. (Variational) Bayesian Inference.
  </figcaption>
  </center>
</figure> 

## Bayesian Neural Network

<figure id="bnn" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/bnn.png"
    alt="A graphical version of Bayesian Neural Networks.">
  <figcaption>
  Figure xxf9. Bayesian Neural Networks.
  </figcaption>
  </center>
</figure> 

## TherML

<figure id="therml" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/therml.png"
    alt="A graphical version of TherML.">
  <figcaption>
  Figure xxf10. TherML.
  </figcaption>
  </center>
</figure> 

## Variational Prediction

<figure id="vp" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/vp.png"
    alt="A graphical version of Variational Prediction.">
  <figcaption>
  Figure xxf11. Variational Prediction.
  </figcaption>
  </center>
</figure> 

## Appendix - Pointwise Bounds

TODO:

 * Reinforcement Learning
 * Learning from human preferences ala. DPO and a density estimation prespective on learning from human feedback.
 * Other Semisupervised or Contrastive learning.
 
 
