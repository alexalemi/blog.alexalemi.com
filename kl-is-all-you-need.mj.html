<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name='viewport' content='width=device-width, initial-scale=1.0' /> 
    <title>blog.alexalemi.com KL is All You Need</title>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-F5SW43T5NT"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-F5SW43T5NT');
    </script>

		<!-- favicon stuff -->
		<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
		<link rel="manifest" href="/site.webmanifest">
		<meta name="msapplication-TileColor" content="#da532c">
		<meta name="theme-color" content="#ffffff">


    <!-- RSS Feed -->
    <link rel="alternate" type="application/rss+xml" title="blog.AlexAlemi.com" href="https://blog.alexalemi.com/rss.xml" />

    <!-- Fonts -->
    <script type="text/javascript">
        WebFontConfig = {
            google: { families: [ 'Muli', 'Lato' ] }
        };
        (function() {
            var wf = document.createElement('script');
            wf.src = ('https:' == document.location.protocol ? 'https' : 'http') + '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
            wf.type = 'text/javascript';
            wf.async = 'true';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(wf, s);
        })();
    </script>

    <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        processEscapes: true
      }};
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Inline CSS -->
    <link rel="stylesheet" type="text/css" href="assets/style.css"/>
</head>

<body>


  <header>
    <h3>Alex Alemi's Blog</h3>
    <nav>
      <a href='https://blog.alexalemi.com' />Index</a> |
      <a href='https://alexalemi.com' />About Me</a> | 
      <a rel="alternate" type="application/rss+xml" title="blog.AlexAlemi.com" href="https://blog.alexalemi.com/rss.xml" />RSS</a>
    </nav>
  </header>

  <article>
		<h1>KL is All You Need</h1>
		<p>Alexander A. Alemi. <time datetime='2024-01-08'>2024-01-08</time></p>
    <div class="content">
    <p>The more I work on machine learning, the more I'm embarrassed to admit that most of what I'm worked on is all so blindingly simple.  At the core of essentially all of modern machine learning is a single
objective: KL minimization, and there is a very simple recipe to follow to re-derive
most of the named objectives out there.
In the past I've discussed some of the
<a href="kl.html">special properties of KL divergence</a>, or how you can derive
<a href="diffusion.html">VAEs or Diffusion Models</a> by means of a simple KL objective, but in
light of a recent <a href="https://www.alexalemi.com/talks/information-theory-for-representation-learning.html">talk</a> I gave at the InfoCog Workshop at NeurIPS 2024, I'd like to prepare essentially a written copy of that talk.<a href="#multivariate-ib"><sup>1</sup></a></p>
<aside> <sup id="figattribution">1</sup>
This is also essentially my own retelling of <a href="https://www.cs.huji.ac.il/labs/learning/Theses/Slonim_PhD.pdf">Noam Slonim's thesis on the Multivariate Information Bottleneck</a>.
</aside>
<figure id="#conditional" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/kl-elephant.png"
    alt="A cartoon depicting several blindfolded scientists analyzing different parts of an elephant, the different scientists think they are looking at VIB, or Diffusion or BNNs or VAEs or Semi-supervised Learning or Bayesian inference, but really its all just KL Divergence.">
  <figcaption>
  Figure 1. The elephant in the room is KL divergence or the relevant entropy.<a href="#figattribution"><sup>2</sup></a>
  </figcaption>
  </center>
</figure> 
<aside> <sup id="figattribution">2</sup>
Cartoon modified from Kevan C. Herold, Jeffrey A. Bluestone ,Type 1 Diabetes Immunotherapy: Is the Glass Half Empty or Half Full?. Sci. Transl. Med.3,95fs1-95fs1(2011).
<a href="https://doi.org/10.1126/scitranslmed.3002981">DOI:10.1126/scitranslmed.3002981</a>.
</aside>
<h2>KL Divergence as Expected Weight of Evidence</h2>
<p>Before we get into it, we need to make sure we're all starting on the same page.  I've written <a href="kl.html">before</a> about part of what makes KL divergence or the relative entropy so special, but for the purposes of the journey we are about to undertake, let's use the interpretation of KL divergence as <a href="kl.html#expected-weight-of-evidence">an expected weight of evidence</a>, which I'll briefly repeat here.</p>
<p>Imagine we have two hypotheses $P$ and $Q$ and we're trying to decide which of these two models is a better model of the world.  We go out an collect some data $D$ and would like to use that data to help us decide between the two models.  Being good probabilistic thinkers with a penchant for gambling, what we're interested in is:</p>
<p>$$ \frac{\Pr(P|D)}{\Pr(Q|D)}, $$</p>
<p>the <a href="https://en.wikipedia.org/wiki/Odds"><i>odds</i></a> of $P$ versus $Q$. Using <a href="https://en.wikipedia.org/wiki/Bayes\%27_theorem">Bayes rule</a> we can express this as:</p>
<p>$$ \frac{\Pr(P|D)}{\Pr(Q|D)} = \frac{\Pr(D|P)}{\Pr(D|Q)} \frac{\Pr(P)}{\Pr(Q)}, $$</p>
<p>the product of the <a href="https://en.wikipedia.org/wiki/Likelihood_function"><i>likelihood</i></a> ratio that the data we observed were generated by model $P$ and $Q$ times the <i>prior odds</i> of the two models.  Taking a logarithm of both sides turns the product into an easier to work with sum:</p>
<p>$$ \log \frac{\Pr(P|D)}{\Pr(Q|D)} = \log \frac{\Pr(D|P)}{\Pr(D|Q)} + \log \frac{\Pr(P)}{\Pr(Q)}. $$</p>
<p>Now, the <i>posterior log odds</i> is expressed as the sum of the <i>weight of evidence</i> plus the <i>prior log odds</i> of the two hypotheses.</p>
<figure id="#belief-of-meter class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/belief-o-meter.png"
    alt="A cartoon representation of a Belief-O-Meter as an old school linear analog meter.">
  <figcaption>
  Figure 2. Belief-O-Meter.
  </figcaption>
  </center>
</figure> 
<p>This <em>weight of evidence</em> tells us how much to update our beliefs in light of evidence.  If you picture a sort of Belief-O-Meter™ for your own beliefs, each bit of independent evidence gives you an additive update for the meter, pushing your beliefs either left or right, towards either $P$ or $Q$.  For simple hypothesis taking the form of probability distributions, this weight of evidence is just the log density ratios of the data under the models:</p>
<p>$$ \log \frac{\Pr(D|P)}{\Pr(D|Q)} \to \log \frac{p(D)}{q(D)}. $$</p>
<p>What then is <a href="https://en.wikipedia.org/wiki/Kullback\%E2\%80\%93Leibler_divergence">the Kullback-Leibler (KL) divergence</a>? Imagine if one of our two hypotheses is actually true.  If $P$ was the probability distribution governing the actual world, the <i>expected weight of evidence</i> we would accumulate from observing some data would be:<a href="#brakets"><sup>3&gt;</sup></a></p>
<p>$$ I[p;q] \equiv \int dx\, p(x) \log \frac{p(x)}{q(x)} \equiv \left\langle \log \frac{p(x)}{q(x)} \right\rangle_{p(x)} . $$</p>
<aside> <sup id="brakets">3</sup>
To clean up the notation, I like using brakets: $ \langle \cdot \rangle_p \equiv \mathbb{E}_{p}[\cdot] \equiv \int dx\, p(x) [\cdot]$, and to clean things up further (or because I'm lazy) I'll often leave off the subscript saying which distribution the brakets are to be taken with respect to, in any of those cases you can assume its a full joint $p$ distribution (over any variables that are otherwise unbound).
</aside>
<p>We can interpret the KL divergence as a measure of how quickly we would be able to discern between hypotheses $P$ and $Q$ if $P$ were true.  Similarly, the <i>reverse KL</i> is:</p>
<p>$$ I[q;p] \equiv \int dx\, q(x) \log \frac{p(x)}{q(x)} \equiv \left\langle \log \frac{p(x)}{q(x)} \right\rangle_{q(x)}, $$</p>
<p>a measure of how quickly we'd be able to discern between $P$ And $Q$ if $Q$ were true.  Suddenly, the asymmetry of the KL divergence, an issue that often causes consternation is no longer a mystery.  We should expect the expected weight of evidence to be asymmetric.   As an extreme example, imagine we were trying to decide between two hypothesis regarding some coin flips we are about to observe.  $P$ is the hypothesis that the coin is fair while $Q$ is the hypothesis the coin is a cheating, double-headed coin.  In this case, if we actually had a fair coin, we expect to be able to perfectly discern the two hypotheses (infinite KL) because we will eventually observe a tails, an impossible situation under the alternative ($Q$) hypothesis.  Meanwhile, if the coin is actually a cheat, we'll be able to collect, on average, 1 bit of evidence per flip in favor of the hypothesis that the coin is a cheat, but we will only ever observe heads and so never be able to perfectly rule out the possibility that the coin is fair and we've simply observed some miracle.<a href="#million"><sup>4</sup></a></p>
<aside> <sup id="million">4</sup>
As a true aside, people often say "one in a million" but lack a good mental model of just how rare that is.  20 heads in a row for a fair coin is a one in a million event. This fun fact and others can be found <a href="https://www.stat.berkeley.edu/~aldous/Real-World/million.html">here, at David Aldous's Home Page</a>.
</aside>
<h2>Mathematical Properties</h2>
<p>In what follows, we'll need to use two mathematical properties of the KL divergence. The first is that the KL divergence is non-negative, i.e. the lowest it can be is zero:</p>
<p>$$ I[p;q] \equiv \int dx\, p(x) \log \frac{p(x)}{q(x)} \geq 0, $$</p>
<p>which I'll leave as an exercise to the reader, or you can see a proof in the <a href="kl.html#non-negative-proof">previous post</a>. In the context of our interpretation of KL divergence as an expected weight of evidence, the non-negativity of KL divergence means, essentially, that the world can't lie to us.  If we are trying to decide between two hypotheses, and one of them happens to be correct, we have to, we must, we have to, we must, on average, be pushed in the direction of the correct hypothesis.</p>
<p>The other property we'll use is the <em>monotonicity</em> of the KL divergence.  This is a generalized version of the <a href="https://en.wikipedia.org/wiki/Data_processing_inequality">data processing inequality</a>.  If we perform some kind of processing on our random variables, it should only make it harder to discern between two hypotheses, not easier.  In particular, the version we'll need today concerns <em>marginalization</em>, if I have two joint distributions defined on two random variables, it always has to be the case that the KL divergence between their two marginals must be less than or equal to the joint KL:
$$ \int dx\, dy\, p(x,y) \log \frac{p(x,y)}{q(x,y)} \geq \int dx\, p(x) \log \frac{p(x)}{q(x)}, $$
which is easy to show if you decompose $p(x,y) = p(x) p(y|x)$ and use the fact that all KL divergences (including the conditional $I[p(y|x);q(y|x)] \geq 0$ are non-negative.</p>
<p>Again, in terms of our current interpretation, this makes sense, if I have some beliefs defined over several variables, if I only get to observe some subset of them, it should be harder for me to discern the beliefs.  The less I look at, the less I see.</p>
<h2>Universal Recipe</h2>
<p>With the prerequisites out of they way we're reading to see the "universal recipe" for generating objectives. Regardless of what we are trying to accomplish, the goal is to make the real world look more like our dreams.  Given that KL divergence is the <em>proper</em> way to measure how similar two distributions are, we need only minimize the KL divergence between the real world -- the world we can sample from -- and the world as we wish it were.  The smaller that KL can become, the harder it becomes for us or anyone else to distinguish between our dreams and reality.  In steps:</p>
<ol>
<li>Draw a causal graphical model corresponding to the world as it is, the true world $P$.</li>
<li>Augment the real world with any components you wish to add.</li>
<li>Draw the world of your desires, what success would look like, what you are targeting, the dream world $Q$.</li>
<li>Minimize $I[P;Q]$.</li>
<li>...</li>
<li>Profit!</li>
</ol>
<p>As simple as it sounds, in retrospect I think a lot of my previous papers were just following this recipe.  Let's repeat this ad nauseam.</p>
<h2>Density Estimation</h2>
<p>We'll start with the problem of density estimation.  Let's say we have some black box that generates samples.  This is the real world $P$, outside of our control.  Despite not knowing how $p(x)$ is structured, we can push the button on the black box to generate samples.  What do we wish for? We wish we instead have a nice description of those same images.  We wish that those images instead came from a box of our own design, some parametric model or probability distribution with knobs that we can adjust to bring it into alignment with the real world, our dream world $q_\theta(x)$ with parameters $\theta$.</p>
<figure id="#density-estimation" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/density-estimation.png"
    alt="A graphical version of density estimation, the left shows the distribution $p(x)$ where $X$ is a random variable denoting images. The right shows the same, but labeled as $q_\theta(x)$.">
  <figcaption>
  Figure 3. Density Estimation.
  </figcaption>
  </center>
</figure> 
<p>Following the recipe, our recipe then is to minimize the KL divergence between the real world and our ideal one:</p>
<p>$$ I[p; q] = \left\langle \log \frac{p(x)}{q_\theta(x)} \right\rangle_p $$</p>
<p>To belabor the point, in terms of our interpretation of KL divergence, this makes sense. $I[p;q]$ measures how easy it is for us to distinguish between $p$ and $q$ using samples from $p$. We have samples from $p$, while $q_\theta(x)$ is a whole set of worlds we can index with our parameters $\theta$.  We seek a setting of those parameters which make it as difficult as possible for us or anyone else to tell the difference between the real world $P$ and our imaginary one $Q$.  Minimizing the KL divergence does exactly that.</p>
<p>Unfortunately, naively, this objective requires that we be able to evaluate $\log p(x)$, the density the real world assigns to the samples it generates.  This is out of reach, we don't know what the real world is doing, but here is where the KL divergence helps us out yet again.  It decomposes into two terms:</p>
<p>$$ \underbrace{\left\langle \log \frac{p(x)}{q_θ(x)} \right\rangle}_{I[p;q]} = \underbrace{\left\langle \log p(x) \right\rangle \vphantom{\left\langle \frac p q \right\rangle} }_{-H[p]} - \underbrace{\left\langle \log q_θ(x) \right\rangle \vphantom{\left\langle \frac p q \right\rangle}}_{H[p;q]},  $$</p>
<p>the (negative) <em>entropy</em> of the true data generating process ($H[p]$), and the <em>cross-entropy</em> between $p$ and $q$: ($H[p;q]$), aka the <em>likelihood</em> of the data samples from $p$ under $q$.  The entropy of the true data generating process isn't something that we control, as far as we're concerned its a constant and we don't need to worry about it. Just like that, we see that minimizing the KL divergence between the real world and the world of our desires, in this simple single random variable setup recovers ordinary minimum <a href="https://en.wikipedia.org/wiki/Cross-entropy">cross-entropy</a> learning, aka maximum likelihood learning, but with a different and hopefully well-motivated origin.  We adjust the parameters of our model $q_\theta(x)$ so as to maximize the likelihood of the data $\log q_\theta(x)$, why? So that we and anyone else would struggle as much as possible to distinguish between the real world and our model.  With this same motivation, lots of other machine learning objectives will fall into place.</p>
<p>There is one caveat, I'm <a href="kl.html#appendix-a">a particular stickler</a> for decomposing KL divergences in this way. I don't think it makes any dimensional sense.  I can't take the logarithm of a dimensional quantity, let alone a density, so I'm going to ruin the flow of this post to address this aside.  To fix the glitch, let's instead try to explicitly choose some tractable base measure $m(x)$ and insert it into our original objective:</p>
<p>$$ \left\langle \log \frac{p(x)}{q_\theta(x)} \right\rangle_p = \left\langle \log \frac{p(x) m(x)}{q_\theta(x)m(x)} \right\rangle = \left\langle \log \frac{p(x)}{m(x)} \right\rangle_p - \left\langle \log \frac{m(x)}{q_\theta(x)} \right\rangle_p . $$</p>
<p>Now, we've decomposed the KL divergence between $P$ and $Q$ into two terms, the first is the KL divergence between $P$ and $M$, our base density. Just as before, this is some constant outside our control. As long as we fix $m(x)$, given that $p(x)$ is fixed, their KL divergence is fixed and no changes we make to $\theta$ have any effect, so we can drop this (now appropriately reparameterization-independent) term from our objective.  We're left with the weight of evidence samples from $p$ provide in favor of $m$ against $q$. If we try to adjust the parameters of $q_\theta(x)$ to make it as easy as possible to distinguish it from some base measure $m(x)$, under samples from $p$, we ensure that we drive $q$ <em>towards</em> $p$.  If we use ordinary path gradients the choice of $m(x)$ here won't actually effect the optimization trajectory. It will, however, help us sleep at night, ensuring that our objective is a truly reparameterization-invariant quantity. <a href="#controlvariates"><sup>4</sup></a></p>
<aside><sup id="controlvariates">4</sup>
If I'm being honest, this is something that bothers me that I don't fully understand.  Using a baseline model here functionally takes the same form as control variates like the baselines used in REINFORCE, but here they don't help at all (we aren't taking an expectation with respect to $q$ here). Regardless, it really feels like an appropriate base measure <i>ought</i> to help.  I can't help but think that it signals a problem with the gradients we take in machine learning today. Things like <a href="https://arxiv.org/abs/2206.07137">RHO-Loss</a> reinforce this idea.
</aside>
<p>TODO Empirical distribution and finite samples.
Since we are already on an aside, I'll go on another aside.  We've just motivated that a useful objective for learning a parametric distribution is to minimize the KL divergence between the true distribution and our parametric distribution, i.e. we should adjust the parameters of our distribution to maximize the likelihood of samples from the true distribution.  In practice however, we typically only have access to a <em>finite</em> number of samples from the true distribution and this introduces a difficulty.  If we wanted to, we could generate an unbiased estimate of the expected likelihood of our model using a finite number of samples from the true distribution:
$$ -\left\langle \log q(x|\theta) \right\rangle_p \approx -\frac 1 N \sum_{i=1}^N \log q(x_i|\theta). $$
Nothing wrong here.  There is similarly nothing wrong with taking the gradient of this monte carlo estimate to generate an unbiased estimate of the gradient of the true likelihood:
$$ -\nabla_\theta \left\langle \log q(x|\theta) \right\rangle_p \approx -\frac 1 N \sum_{i=1}^N \nabla_\theta \log q(x_i|\theta). $$
The problem only occurs if we start to <em>reuse</em> the same samples.  These monte carlo estimates are only <em>unbiased</em> estimates of the true expectation if the samples are independent.  If we start to take multiple gradient steps with overlapping samples we start to introduce some bias.  Taken to the extreme, if we simply maximize the <em>empirical</em> likelihood on a fixed set of finite samples:
$$ \sum_{i=1}^N \log q(x_i|\theta), $$
We are no longer minimizing the KL divergence between the <em>true</em> distribution $p(x)$ and our parametric distribution $q(x|\theta)$, instead we are minimizing the KL divergence between the <em>empirical</em> distribution $\hat p$ and our parametric distribuiton $q(x|\theta)$:
$$ \hat p \equiv \frac 1 N \sum_{i=1}^N \delta(x - x_i). $$
If we had a very large number of samples, this empirical estimate would be pretty close to our true $\hat p \sim p$, but with finite samples it is always a distinct distribution from the true.  If we minimize the empirical risk, or maximize the empirical likelihood what we are really doing is getting our parametric distribution to be as indistinguishable as possible from the empirical distribution. This is equivalent to saying we should match sampling with replacement from our training set.  This is really where all of the issues of overfitting come from.  The degree to which matching the empirical distribution rather than the true distribution is a problem depends on how little data we have (relative to its sort of extent or coverage) and how flexible our parametric model is (the degree to which it can memorize the data we show it and nothing else).  In the context of classical machine learning this is where <em>regularization</em> comes to bear, we typically add some additional terms to our objective beyond just the empirical likelihood to attempt to get our learned model to better approximate the true distribution rather than the empirical.</p>
<p>I want to acknowledge that this is a problem, but in the context of the current discussion I want to point out that this <em>isn't</em> a problem with our <em>objective</em>.  It is a good idea to try to minimize the KL divergence between the true distribution and our parametric model.  After we decide on this objective, unfortunately, there are practical issues we have to consider about how to target this objective tractably and accurately.</p>
<h2>Supervised Learning</h2>
<p>Let's complicate things slightly.  Instead of imagining that we have a single random variable in the real world, imagine instead we have a pair of variables, $X$ and $Y$.  For concreteness, imagine the $X$ are images and the $Y$ are their associated labels in some dataset.</p>
<p>What are we after? What does success look like? Let's imagine that what we desire is the ability to assign labels to data.  What we wish were the case was that we used the same process to draw the images $q(x) = p(x)$, but instead of using the real world process to assign labels, ideally the labels would instead come from a device under our control: $q_\theta(y|x)$. <a href="#theta"><sup>5</sup></a>.  Just as before, we simply minimize the KL divergence between these two joints and we obtain an objective:</p>
<figure id="#supervised-learning" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/supervised-learning.png"
    alt="A graphical version of supervised learning, the left shows the distribution $p(y,x)$ where $X$ is a random variable denoting images and $Y$ are some labels. The right shows the world of our dreams, where we draw the images $X$ from the same process as in the real world $p(x)$ but we have a machine that applies the labels: $q(y|x)$.">
  <figcaption>
  Figure 4. Supervised Learning.
  </figcaption>
  </center>
</figure> 
<aside> <sup id="theta">5</sup>
I'm going to start dropping the subscript $\theta$ for the parameters.
</aside>
<p>$$ \left\langle \log \frac{p(x,y)}{p(x)q(y|x)} \right\rangle, $$</p>
<p>Just as above, when we drop constants outside of our control, we end up with the usual maximum likelihood objective we are used to:</p>
<p>$$ \left\langle \log \frac{p(x)p(y|x)}{p(x)q(y|x)} \right\rangle = \left\langle \log \frac{p(y|x)}{q(y|x)} \right\rangle. $$
With the same caveats about proper handling of dimensions and issues stemming from using a fixed set of finite samples.</p>
<h2>Variational Autoencoders</h2>
<figure id="vae" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/vae.png"
    alt="A graphical version of a VAE.">
  <figcaption>
  Figure 5. Variational Autoencoders.
  </figcaption>
  </center>
</figure> 
<h2>Variational Information Bottleneck</h2>
<figure id="vib" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/vib.png"
    alt="A graphical version of VIB.">
  <figcaption>
  Figure 6. Variational Information Bottleneck.
  </figcaption>
  </center>
</figure> 
<h2>Diffusion</h2>
<figure id="diffusion" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/diffusion.png"
    alt="A graphical version of Diffusion.">
  <figcaption>
  Figure 7. Variational Diffusion.
  </figcaption>
  </center>
</figure> 
<h2>Semi-Supervised Learning</h2>
<figure id="semi-supervised" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/semi-supervised.png"
    alt="A graphical version of a Semi-supervised VAE.">
  <figcaption>
  Figure 8. Semi-Supervised Variational Autoencoder.
  </figcaption>
  </center>
</figure> 
<h2>Bayesian Inference</h2>
<figure id="bayes" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/bayes.png"
    alt="A graphical version of Bayesian Inference.">
  <figcaption>
  Figure 9. (Variational) Bayesian Inference.
  </figcaption>
  </center>
</figure> 
<h2>Bayesian Neural Network</h2>
<figure id="bnn" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/bnn.png"
    alt="A graphical version of Bayesian Neural Networks.">
  <figcaption>
  Figure 10. Bayesian Neural Networks.
  </figcaption>
  </center>
</figure> 
<h2>TherML</h2>
<figure id="therml" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/therml.png"
    alt="A graphical version of TherML.">
  <figcaption>
  Figure 10. TherML.
  </figcaption>
  </center>
</figure> 
<h2>Variational Prediction</h2>
<figure id="vp" class="right">
  <center>
  <img width="95%" src="figures/kl-is-all-you-need/vp.png"
    alt="A graphical version of Variational Prediction.">
  <figcaption>
  Figure 11. Variational Prediction.
  </figcaption>
  </center>
</figure> 
<h2>Appendix - Pointwise Bounds</h2>
<p>TODO:</p>
<ul>
<li>Reinforcement Learning</li>
<li>Learning from human preferences ala. DPO and a density estimation perspective on learning from human feedback.</li>
<li>Other Semi-supervised or Contrastive learning.</li>
</ul>
 
    </div>
  </article>

  <script src="https://giscus.app/client.js"
        data-repo="alexalemi/blog.alexalemi.com"
        data-repo-id="MDEwOlJlcG9zaXRvcnkyNjk2OTU4MzU="
        data-category="Announcements"
        data-category-id="DIC_kwDOEBM7W84B_4Ke"
        data-mapping="pathname"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-theme="light"
        data-lang="en"
        crossorigin="anonymous"
        async>
  </script>

  <footer>
    <!-- <p>
    &copy; 2020 Alexander A. Alemi
    </p>
    -->
  </footer>



</body>
</html>