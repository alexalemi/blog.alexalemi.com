# TODO

Some post ideas.

 * KL is all of machine learning.
 * The Path we are On.
 * KL and M&Ms.
 * Promise of Deep Learning and Variational Approaches.
 * Color the bits.
 * Intro to Thermodynamics
 * Overview of Statistics
 * Introduction to Dimensional Analysis (the Stefan Boltzmann Equation)

 * AI for Hive
 * AI for Lost Cities
 * AI for Cartographers
 * Fermi

Collections of notes and drafts of potential blog posts on topics.

# Random list of ideas.

 - Things to include.
 - Information criterion.
 - Correspondence
 - Sloppy models
 - Bootstrap
 - Thermo
 - Coin optimal. Model mismatch. Duplicated data.
 - Thermodynamics of information
 - Use mutual information to reduce entropy
 - Alright, current thoughts, definitely reread and work through the Carrol paper.
Try to apply it in my case for new data.
 - Other thoughts, both N and the temperature are important as they characterize
independent axes of variation. In the extreme case we can have a per particle
chemical potential if we let the quenched data vary.
 - Need to also incorporate AIS into story, as well as NTK stuff.
 - New linear paper might give a tractable Fokker Planck equation and S as a
result. Then we'd have exact info plane Dynamics.
 - Mutual information, do I want this optimal can I equate the results and scaling
of both predictive info and rational ignorance to get a atoms ~ N^(2p/3) type
scaling? Seems pretty oppressive quickly.
 - Other thoughts G the generalization error really is like a Gibbs free energy
since it's a chemical potential that you are minimizing. Can use this to
identify the pressure and volume. Also can help try to resolve some issues of
nonextensivity perhaps.
 - I was thinking this morning more about the blog posts, I have a sort of sequence in mind:

 * The Promise of Deep Learning

  The world has changed.  We have gotten orders of magnitude better at searching over
  function space.  For anything phrased as a variational problem, we can suddenly do much
  better at it than before.  Some examples (NeRF, Lagrangian Mechanics, AlphaZero).
  
  Want to think some more about mean field variational principle.
  
  Also broadly thought before we can use this to probe theories better, fit the
  residual, does that actually work, can we figure that out?
  
 * Why KL?  Bayes Rule isn't as trivial as it seems, KL is uniquely the only
   one that decomposes, and if we are going to have Bayes rule we need KL.
   Should work that out.

 * The Hunt for Functionals.  I really wanna find the true learning functional,
   could work by analogy with the one in physics.
  
 * GPT-3 and thoughts on big models.  Presumably the big models aren't actually
   magical and are instead utilizin some kind of regularization property to
   make them awesome, could explore this in an infinite tractable dataset.

 * Bayes isn't what its purported to be.  Explanation versus Prediction.  Bayes with model mismatch.

 * Perception of probabilities, my survey and stuff.

 * Summary of my papers.

 * Thermodynamics

 * minGPT

 * Dimensional analysis

 * Units of Likelihoods

 * User guide for VIB

 * Information projection as a way of getting objectivesInformation projection as a way of getting objectives.

 * Dreams and Constrastive divergence.  Make a neat little toy model that tries to learn a game or something like a world model, with wake sleep dreaming stuff.

 * Why we remember the past and not the future.

 * Can aliens hear us?

 * Thermodynamics.
* Blotto v2
* VAE
* VIB
* Why mutual information?
* Mapping CIFAR to MNIST
* PIB for falling body
* ML as function search
* Coronavirus
* Projective geometric algebra
* Fake data
* Energy model in classifier
* Climate science
* Portfolio visualization
* Identicons
* Fermi
* Universal statistics
* Letters to Alex. My ideas of what papers are and what they should be.
* You don't know VAEs
 * You don't know Bayes (PAC Derivation)
 * Exponential families as min KL
 * Polar Codes
 * Weight of Evidence
 * Summary of Rational Ignorance
 * Why blogging works
 * Dimensional Analysis done better.
 * Quantum decoherence and Cats. 
    Look at the problem in the Quantum Problem Solver
 * Whats the deal with BIC / AIC / WAIC / QIC ?
 * How random are sports?
 * Missing middle of science education
 * Earth gravitational field
 * Time travel coordinates
 * Modelling all of the bits.
 * Taking KLs between joints as all of ML.
 * Nomograms
 * PGA.
 * Quantum probability. Making sense of manfred's thing.
 * Kelly Criterion and its applications.
 * Color the Bits. and GPT3
 * 1D Projective Line and Bits as Units.
 * Pascal, kids, magnitude and studies.
 * sexibels, decibels, 1-2-5 all that.
 * Difference between inference and prediction, PACm and friends.
 * A rant on representation learning. (color the bits)
 * Say the loud part out loud, we should model the things we believe exist.
 * Thinking about climate change, temperature variations by latitude
 * Dimensional analysis through the lens of black body radiation and COBE CMB.
 * KL from M&Ms 
 * Basic Statistics from M&Ms 
 * Bayes from M&Ms 
 * Biflow VAEs, conditional bijector stuff 
 * exponential families as solution to a problem.
 * really work out in detail the philosophy of VAE or VIB.
 * VIB++
 * Dual version of bayes.
 * Joints are where it's at, theories.
