<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Blog.AlexAlemi.com</title><link>https://blog.alexalemi.com/rss.xml</link><description>Various musings.</description><atom:link href="https://blog.alexalemi.com/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>http://blog.alexalemi.com/favicon.ico</url><title>Blog.AlexAlemi.com</title><link>https://blog.alexalemi.com/rss.xml</link></image><language>en</language><lastBuildDate>Tue, 21 Jun 2022 02:17:18 +0000</lastBuildDate><item><title>Why KL?</title><link>https://blog.alexalemi.com/kl.html</link><description>Why is the KL divergence so special?</description><content:encoded>&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence"&gt;Kullback-Liebler
divergence&lt;/a&gt;,
or KL divergence, or relative entropy, or relative information, or information
gain, or expected weight of evidence, or information divergence
(it goes by a lot of different names) is unique
among the ways to measure the difference between two probability
distributions.  It holds a special and privileged place, being used to define
all of the core concepts in information theory, such as mutual information.&lt;/p&gt;
&lt;p&gt;Why is the relative information so special and where does it come from?
How should you interpret it? What is a nat anyway?  In this
note, I'll try to give a better understanding and set of intuitions about
what KL is, why it's interesting, where it comes from and what it's good for.&lt;/p&gt;
&lt;h2&gt;Information Gain&lt;/h2&gt;
&lt;p&gt;Let's see if we can motivate the form of the KL axiomatically.&lt;/p&gt;
&lt;p&gt;Imagine we have some prior set of beliefs summarized as a probability distribution $q$.
In light of some kind of evidence, we update our beliefs to a new distribution $p$.
How &lt;em&gt;much&lt;/em&gt; did we update our beliefs?  How do we quantify
the &lt;em&gt;magnitude&lt;/em&gt; of that update?  What are some properties we might want this
hypothetical function to have?  Let $I[p; q]$ denote the function that measures
how much we moved beliefs when we switch from beliefs $q$ to beliefs $p$.  We'll
call this amount of update the &lt;em&gt;information gain&lt;/em&gt; when we move from $q$ to $p$.
&lt;sup&gt;&lt;a href="#hobson"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;aside&gt; &lt;sup id="hobson"&gt;1&lt;/sup&gt; 
  What follows is my own reconstruction of the fabulous paper: 
  &lt;a href="https://link.springer.com/article/10.1007/BF01106578"&gt;
  &lt;b&gt;A New Theorem of Information Theory&lt;/b&gt; by Arthur Hobson
  &lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;We want our information function to satisfy the following properties:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It's &lt;strong&gt;continuous&lt;/strong&gt;.  A small change in the distributions makes a small change in the amount of information in the move.&lt;/li&gt;
&lt;li&gt;It's permutation or &lt;strong&gt;reparameterization independent&lt;/strong&gt;.  It doesn't matter if we change the units we've specified our distributions in or if we relabel the sides of our dice, the answer shouldn't change.&lt;/li&gt;
&lt;li&gt;We want it to be &lt;strong&gt;non-negative&lt;/strong&gt; and have the value $I = 0$ if and only if $p = q$.  If $p=q$ we haven't updated our beliefs and so have no information gain.&lt;/li&gt;
&lt;li&gt;We want it to be &lt;strong&gt;monotonic&lt;/strong&gt; in a natural sense.  If we, for instance, start with some uniform distribution over the 24 people in a game of &lt;a href="https://en.wikipedia.org/wiki/Guess_Who%3F"&gt;Guess Who?&lt;/a&gt; and then update to only 5 remaining suspects, $I$ should be larger than if there were still 12 remaining suspects.&lt;/li&gt;
&lt;li&gt;Finally, we want our information function to &lt;strong&gt;decompose&lt;/strong&gt; in a natural and &lt;strong&gt;linear&lt;/strong&gt; way.&lt;sup&gt;&lt;a href="#renyi"&gt;2&lt;/a&gt;&lt;/sup&gt; In particular, we want to be able to relate the information between two joint distributions in terms of the information between their marginal and conditional distributions.&lt;/li&gt;
&lt;/ol&gt;
&lt;aside&gt; &lt;sup id="renyi"&gt;2&lt;/sup&gt;
  If one relaxes the requirement for linear decomposition and instead just requires that our information
  function decompose in a convex way, you get the generalized set of 
  &lt;a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy#R%C3%A9nyi_divergence"&gt;Rényi divergences&lt;/a&gt;.
  See: &lt;a href="https://projecteuclid.org/euclid.bsmsp/1200512181"&gt;
  &lt;i&gt;On Measures of Entropy and Information&lt;/i&gt; by Alfréd Rényi.&lt;/a&gt;
&lt;/aside&gt;
&lt;p&gt;These are all very natural properties for our information function to have.  That last point about composition needs to be elaborated.
The point is that we have alternative ways we might express a probability distribution.  Apropos of nothing, imagine we
are concerned that we might have been exposed to a disease and are thinking about getting a test done.  There are two random variables
under consideration, we will label them $\mathcal{D}$ for whether we actually had the disease or not,
and $\mathcal{T}$ for whether
the test result is positive.  Each of these random variables can take on two possible states, we'll denote them as
$\mathcal{D} \in \{ D, \overline D \}, \mathcal{T} \in \{ T, \overline T \}$.
$D$ represents the state of our having-had-the-disease random variable $\mathcal{D}$ being positive, meaning we actually
did have the disease.  $\overline D$ denotes we actually didn't.
With two binary random variables, there are 4 possible outcomes $(\{ DT, D\overline T, \overline D T, \overline D \overline T\})$
and fully specifying our set of beliefs requires 3 independent probabilities.&lt;/p&gt;
&lt;aside&gt; &lt;sup id="kent"&gt;3&lt;/sup&gt;
  An &amp;ldquo;&lt;i&gt;Almost Certainly Not&lt;/i&gt;&amp;rdquo; is 7% on
  the &lt;a href="https://en.wikipedia.org/wiki/Words_of_estimative_probability"&gt;Kent's words of Estimative Probability&lt;/a&gt; list.
&lt;/aside&gt;
&lt;aside&gt; &lt;sup id="covid"&gt;4&lt;/sup&gt;
  See for instance the RDT Cellex Inc. &lt;a href="https://www.centerforhealthsecurity.org/resources/COVID-19/serology/Serology-based-tests-for-COVID-19.html"&gt;SARS-COV-2 Test&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;What are our prior beliefs?
Let's imagine while we are concerned we might have had the disease, but if we are being honest,
we almost certainly didn't,&lt;sup&gt;&lt;a href="#kent"&gt;3&lt;/a&gt;&lt;/sup&gt;
so we'll put our prior belief in having had the disease at 7%. $(q(D) = 0.07)$.
How do we expect the antibody test to go if we have it done?
You do a bit of research and discover
that if you had had the disease, the sensitivity or &lt;em&gt;true positive rate&lt;/em&gt; of the
test you're about to take is 93.8% $(q(T|D) = 0.938)$.
The specificity or &lt;em&gt;true negative rate&lt;/em&gt; of that
same test is 95.6% $(q(\overline T | \overline D) = 0.956)$. &lt;sup&gt;&lt;a href="#covid"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;figure id="#conditional" class="right"&gt;
  &lt;center&gt;
  &lt;img width="45%" src="figures/KLdiagram2.svg"
    alt="Conditional characterization of distribution."&gt;
  &lt;img width="45%" src="figures/KLdiagram.svg"
    alt="Joint characterization of distribution."&gt;
  &lt;figcaption&gt;
  Figure 1. Two equivalent ways to express the joint distribution $q(\mathcal{D}\mathcal{T})$.
  &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
We've just specified our prior beliefs with 3 numbers, imagining our process as having two steps,
first, we either had the disease or not $(q(\mathcal{D}))$ and then, conditioned on that
we get the result of our test $(q(\mathcal{T}|\mathcal{D}))$.  
Equivalently, we could have just given the joint probability distribution, as shown in Figure 1.
&lt;p&gt;The point now is that if we were to update our beliefs, in the diagram on the right there is just a single
distribution $q(\mathcal{D},\mathcal{T})$, in the one on the left there are essentially three different distributions
$(q(\mathcal{D}), q(\mathcal{T}|D), q(\mathcal{T}| \overline D))$ and we want
some sort of &lt;em&gt;structural&lt;/em&gt; consistency between the two sides:
$$
I[p(\mathcal{D},\mathcal{T}); q(\mathcal{D},\mathcal{T})] \quad \textrm{versus} \quad
I[p(\mathcal{D}); q(\mathcal{D})], I[p(\mathcal{T}|D); q(\mathcal{T}|D)],
I[p(\mathcal{T}|\overline D), q(\mathcal{T}|\overline D)] . 
$$&lt;/p&gt;
&lt;p&gt;The consistency we will require is that our information measure decomposes linearly between
these two different descriptions. The information between the joints should be a weighted
linear combination of the informations of three constituent distributions.
In this particular case we will require:
$$ I[p(\mathcal{D},\mathcal{T}); q(\mathcal{D},\mathcal{T})] =  I[p(\mathcal{D}); q(\mathcal{D})] + p(D) I[p(\mathcal{T}|D); q(\mathcal{T}|D)] + p(\overline D) I[p(\mathcal{T}|\overline D), q(\mathcal{T}|\overline D)] .
$$
In words: The information in the full joint update is the information update for
your belief in whether or not you had the disease $(q(\mathcal D))$ &lt;em&gt;plus&lt;/em&gt; the informations
in the two conditional distributions, but weighted by how often we find ourselves in each of those
branches, as measured by our updated beliefs $(p(\mathcal{D}))$.&lt;/p&gt;
&lt;p&gt;More generally we are requiring that our information function satisfies a natural &lt;em&gt;chain rule&lt;/em&gt;:
$$ I[ p(X,Y); q(X,Y) ] = I[ p(X); q(X) ] + \mathbb{E}_{p(X)} \left[ I[ p(Y|X); q(Y|X) ] \right] $$&lt;/p&gt;
&lt;p&gt;Notice that it is here, in this sort of structural independence that we make
our information function manifestly asymmetric.  Here our $p$ distribution
becomes distinguished over our $q$ as it is the one we use to weight the child
contributions.  This makes sense if we imagine or if $p$ is the actual
distribution that events are drawn from, for it means that this will correspond
to the information we would observe in expectation.&lt;/p&gt;
&lt;p&gt;The interesting thing is that if you want your information function to satisfy
all of these seemingly reasonable properties, that is enough to determine it
&lt;em&gt;uniquely&lt;/em&gt;.  The only function satisfying all of these properties is the
relative entropy, or KL divergence we all know and love:
$$
I[p;q] = \int \mathrm dx\, p(x) \log \frac{p(x)}{q(x)}
$$&lt;/p&gt;
&lt;p&gt;See &lt;a href="https://link.springer.com/article/10.1007/BF01106578"&gt;
&lt;b&gt;A New Theorem of Information Theory&lt;/b&gt; by Arthur Hobson
&lt;/a&gt; for a complete proof,
but here I'll offer a more colloquial argument like the one
given by Ariel Caticha.&lt;sup&gt;&lt;a href="#caticha"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;aside&gt; &lt;sup id="caticha"&gt;5&lt;/sup&gt;
  &lt;i&gt;Lectures on Probability, Entropy and Statistcal Physics&lt;/i&gt; by
  Ariel Caticha. &lt;a href="https://arxiv.org/abs/0808.0012"&gt;arXiv:0808.0012&lt;/a&gt;
&lt;/aside&gt;
&lt;p&gt;We will start with and focus on the continuous setting, where we have two probability
distributions $p$ and $q$.  We seek a functional that takes our two distributions
and gives back our information gain and we seek one that is &lt;em&gt;local&lt;/em&gt; in the physics sense,
meaning that our &lt;em&gt;functional&lt;/em&gt; can be written as the integral of a &lt;em&gt;function&lt;/em&gt; depending
only on the values the probability densities take at each point:
$$ I[p;q] = \int \mathrm dx\, \mathcal{A}(x, p(x), q(x)). $$&lt;/p&gt;
&lt;p&gt;Our requirement that our information gain be
&lt;em&gt;reparameterization independent&lt;/em&gt; means it has to
be invariant to any remapping of our coordinates, or in other words,
it has to be dimensionless.  Imagine $x$ has units of a length, here our integral
measure $\mathrm dx$ has units of a length, and the densities $p(x), q(x)$ would
have units of an inverse length.  In order to be dimensionally consistent
our functional must take the form:&lt;sup&gt;&lt;a href="#caveat"&gt;6&lt;/a&gt;&lt;/sup&gt;
$$ I[p;q] = \int \mathrm dx\, p(x) f\left( \frac{p(x)}{q(x)} \right). $$&lt;/p&gt;
&lt;aside&gt; &lt;sup id="caveat"&gt;6&lt;/sup&gt;
  We could have just as well written it as $I[p;q] = \int \mathrm dx\, q(x) g\left( \frac{p(x)}{q(x)} \right)$ (that is, the form
  of an &lt;a href="https://en.wikipedia.org/wiki/F-divergence"&gt;f-divergence&lt;/a&gt;), but 
  this is equivalent to the way we wrote it with $f(\mathcal{X}) = \mathcal{X} g(\mathcal X)$.
  Putting the $p(x)$ as the integral measure better aligns with what we are about to do next.
&lt;/aside&gt;
&lt;p&gt;Finally, our decomposability requirement above when written out in terms of
continuous densities takes the form:
$$ I[ p(x,y); q(x,y) ] = I[ p(x); q(x) ] + \int \mathrm dx\, p(x) I[p(y|x) ; q(y|x)] $$&lt;/p&gt;
&lt;p&gt;Combining this linear decomposition requirement with our requirement for the
form required and pushing some equations around gives us:
$$
\begin{align}
I[ p(x,y); q(x,y) ] &amp;amp;= I[p(x); q(x)] + \int \mathrm dx\, p(x) I[p(y|x); q(y|x)] \\
\int \mathrm dx\, \mathrm dy\, p(x,y) f\left(\frac{p(x,y)}{q(x,y)} \right)&amp;amp;= \int \mathrm dx\, p(x) f\left(\frac{p(x)}{q(x)} \right) + \int \mathrm dx\, p(x) \int dy\, p(y|x) f\left(\frac{p(y|x)}{q(y|x)} \right) \\
\int \mathrm dx\, \mathrm dy\, p(x) p(y|x) f\left(\frac{p(x)p(y|x)}{q(x)q(y|x)} \right)&amp;amp;= \int dx\, dy\, p(x) p(y|x) \left[ f\left(\frac{p(x)}{q(x)} \right) + f\left(\frac{p(y|x)}{q(y|x)} \right)\right] .
\end{align}
$$
Notice that this demonstrates that our function $f$ must satisfy the property:
$$ f(ab) = f(a) + f(b). $$
This well known functional equation has a unique (up to a multiplicative constant) &lt;em&gt;continuous&lt;/em&gt; solution:
$$ f(x) = c \log x. $$
We can roll the choice of multiplicative constant into our choice of basis for the logarithm and arrive at our final form
for our information gain:
$$ I[p;q] = \int \mathrm dx\, p(x) \log \frac{p(x)}{q(x)}. $$&lt;/p&gt;
&lt;p&gt;As for the non-negativity, our final form satisfies that property.  Because we have that $\log x \leq x -1$:
$$ I[p;q] = \int \mathrm dx\, p(x) \log \frac{p(x)}{q(x)} = -\int \mathrm dx \, p(x) \log \frac{q(x)}{p(x)} \geq
-\int \mathrm dx\, p(x) \left( \frac{q(x)}{p(x)} - 1 \right) = 0. $$&lt;/p&gt;
&lt;aside&gt;
  &lt;img width="100%" src="figures/logbound.svg"
    alt="Visual demonstration of log x &lt; x - 1."&gt;
&lt;/aside&gt;
&lt;h2&gt;Bayes Rule&lt;/h2&gt;
&lt;aside&gt; &lt;sup id="caticha2"&gt;7&lt;/sup&gt;
  I first saw this form of motivation for Bayes Rule in
  &lt;i&gt;Lectures on Probability, Entropy and Statistical Physics&lt;/i&gt; by
  Ariel Caticha. &lt;a href="https://arxiv.org/abs/0808.0012"&gt;arXiv:0808.0012&lt;/a&gt;
&lt;/aside&gt;
Having identified the right way to measure how much information is gained when we update a distribution
from $q$ to $p$, why don't we put this to practical use and try to figure out how we 
&lt;i&gt;ought&lt;/i&gt; to update
our beliefs in light of evidence or observations.&lt;sup&gt;&lt;a href="#caticha2"&gt;7&lt;/a&gt;&lt;/sup&gt;
&lt;p&gt;Returning to our disease testing example, let's say you get the test done and receive a
positive result $(\mathcal T = T)$.
What should your new distribution of beliefs be? Well, first off if we've observed the results of the test
we should probably have our updated beliefs reflect the observation we made, making it consistent with our
observation, setting $p(T) = 1$, but this doesn't fully specify $p$; we need two more numbers.  How should
we set those?&lt;/p&gt;
&lt;p&gt;Why don't we aim to be conservative and try to find a new set of beliefs
that are as close as possible to our prior beliefs while still being consistent with the
observation that we've made?&lt;br /&gt;
Namely, let's look now for a joint distribution $p(\mathcal T, \mathcal D)$
that is as close as possible to $q(\mathcal T, \mathcal D)$ but for which we have that $p(T)=1$.
$$ \DeclareMathOperator{\argmin}{arg\,min} $$
$$ \argmin_{p(\mathcal D, \mathcal T)} I[p(\mathcal D, \mathcal T); q(\mathcal D, \mathcal T)] \quad \text{ s.t. }\quad p(T) = 1 $$
Now that we know
how to measure how much information is gained in updating our beliefs, we will
find the $p$ that minimizes this update while still being true to the observation we made.
Writing $p(\mathcal D,\mathcal T) = p(\mathcal T)p(\mathcal D|\mathcal T)$
and using our linear decomposition rule from above (the other way around), we have:
$$ I[p(\mathcal D,\mathcal T); q(\mathcal D,\mathcal T)] = I[p(\mathcal T);q(\mathcal T)] + I[p(\mathcal D|T);q(\mathcal D|T)]. $$
Because we've decided to fix $p(T)=1$ in order to be consistent with our
observation, the way to minimize the information between the joints is to set $p(\mathcal D|T)=q(\mathcal D|T)$ so
that our second term vanishes. In this particular case this means:
$$ p(T)=1 $$
$$ p(D|T) = q(D|T) = \frac{q(T|D)q(D)}{q(T|D)q(D) + q(T|\overline D)q(\overline D)} = 0.616 $$&lt;/p&gt;
&lt;p&gt;Furthermore, the marginal distribution of our updated beliefs about our disease status is:
$$ p(D) = p(D|T)p(T) = q(D|T) = 0.616$$
In this particular case our updated belief is only 3 to 2 on
that we actually had the disease, despite our positive test result. In Figure 2
we show both our prior in this factorization as well as our new beliefs.&lt;/p&gt;
&lt;figure id="#posterior" class="right"&gt;
  &lt;center&gt;
  &lt;img width="35%" src="figures/KLdiagram2q.svg"
    alt="Prior distribution of beliefs."&gt;
  &lt;img width="35%" src="figures/KLdiagram2p.svg"
    alt="Posterior distribution of beliefs."&gt;
  &lt;figcaption&gt;
  Figure 2. Our prior (left, blue, notice that we've swapped the order of the conditioning) and updated (right, orange) beliefs after observing that the test was positive.
  &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Notice what just happened.  If we look for a new distribution that is as close as possible
to our previous distribution of beliefs (as measured by $I[p;q]$) which is also consistent
with our observations, we end up with an updated, or &lt;em&gt;posterior&lt;/em&gt; set of beliefs given
by Bayes' Rule.  Imagine we had some observable $x$ and some parameters $\theta$.  Our
prior set of beliefs are described by the joint distribution $q(\theta,x) = q(x|\theta)q(\theta)$:
a &lt;em&gt;likelihood&lt;/em&gt; $q(x|\theta)$ of how we expect the data to be distributed given
the parameter values and some &lt;em&gt;prior&lt;/em&gt; $q(\theta)$ set of beliefs about what values
those parameters can take.  If we make an observation and see some value for our observable $x=X$,
what ought our new beliefs be?  If we search for the joint distribution $p(x,\theta)$ that is
as close as possible to our previous beliefs $q(x,\theta)$ but that no longer has any
uncertainty about the value the observable will take $(p(x) = \delta(x-X))$ we see
that minimizing the information gain:
$$ I[p;q] = I[p(x);q(x)] + \int \mathrm dx\, p(x) \, I[p(\theta|x); q(\theta|x)], $$
is accomplished if we set $p(\theta|x) = q(\theta|x)$, yielding the updated joint:
$$ p(x,\theta) = p(x)p(\theta|x) = \delta(x-X) q(\theta|x) $$
and the marginal beliefs about the parameters to be:
$$ p(\theta) = \int \mathrm dx\, p(x,\theta) = \int \mathrm dx\, \delta(x-X) q(\theta|x) = q(\theta|X), $$
or precisely what you probably thought it should have been anyway if you've heard
of Bayesian inference.&lt;/p&gt;
&lt;p&gt;Although, if you stop to think about it, even though many of us know of and have
used &lt;a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"&gt;Bayes Theorem&lt;/a&gt;
for a long time, the way it's normally presented, it is just a trivial statement
about how joint distributions factor.
$$ q(\theta, D) = q(\theta) q(D|\theta) = q(D) q(\theta|D)  \implies
q(\theta|D) = \frac{q(D|\theta) q(\theta)}{q(D)}. $$
But, this is just a statement about distribution
$q$, our prior beliefs.  It tells us nothing about how we should update those
beliefs in light of observations.  However, the previous argument demonstrates
that if you want to set your updated beliefs such that they are as close
as possible to your prior beliefs while being consistent with your
observations, you should set your updated beliefs according to
Bayes' rule run on the prior beliefs.&lt;/p&gt;
&lt;h2&gt;Expected Weight of Evidence&lt;/h2&gt;
&lt;p&gt;Traditionally, KL is interpreted from a coding perspective, a view I've included in an appendix below,
but here I offer a different perspective from the viewpoint of model selection.&lt;/p&gt;
&lt;p&gt;Above we saw that we can motivate Bayesian inference as choosing a posterior belief distribution
that has the minimal information gain over our prior distribution of beliefs while being consistent
with our observations.  This guides us towards forming better belief distributions, but what if we
just have two different belief distributions and wish to decide between them?&lt;/p&gt;
&lt;p&gt;Really what we want to know is what is the probability that our beliefs are correct in light of evidence?
Symbolically you might write this as $p(P|E)$ where $P$ is some belief distribution and $E$ is some
evidence, data, or observations. If we run Bayes Theorem we can see that:
$$ p(P|E) = \frac{p(E|P) p(P)}{p(E)}. $$
We can update our belief in our beliefs being correct by setting our updated
weight in the belief $p(P|E)$ to be proportional to our initial weight $p(P)$ times
the &lt;em&gt;likelihood&lt;/em&gt; that the evidence we observed would have been generated if our belief was true $(p(E|P))$.  The probability of the evidence given the belief $P$ is just the likelihood $P(E)$.
Proportional because we would need to know how likely the evidence would be $p(E)$ amongst all possible
beliefs. This last part, the &lt;a href="https://en.wikipedia.org/wiki/Marginal_likelihood"&gt;&lt;i&gt;marginal likelihood&lt;/i&gt;&lt;/a&gt;
is notoriously difficult to compute. In principle, it is asking us to evaluate how likely
the evidence would be from all possible models.&lt;/p&gt;
&lt;p&gt;However, we can make further progress if we content ourselves to not necessarily knowing the
absolute probability our model or beliefs are correct, but instead just its probability relative
to some other model.  If we consider the ratio of two different models $P$ and $Q$ we have:
$$ \frac{p(P|E)}{p(Q|E)} = \frac{p(E|P)}{p(E|Q)} \frac{p(P)}{p(Q)}. $$
Notice that the marginal likelihoods cancel out.  This is saying that whatever prior relative odds for the two models
being correct, if we compute the &lt;a href="https://en.wikipedia.org/wiki/Bayes_factor"&gt;&lt;i&gt;Bayes factor&lt;/i&gt;&lt;/a&gt;
$\left( \frac{p(E|P)}{p(E|Q)} \right)$, it tells us how the relative probabilities of the two beliefs should update
in light of the evidence. Taking a log on both sides:
$$ \log \frac{p(P|E)}{p(Q|E)} = \log \frac{p(E|P)}{p(E|Q)} + \log \frac{p(P)}{p(Q)},$$
turns this multiplicative factor into an additive one.&lt;/p&gt;
&lt;p&gt;If what we are deciding between is two different probability distributions, you may recognize that this additive &lt;i&gt;weight of evidence&lt;/i&gt;
for $p$ over $q$ when we observe $x$ is precisely the integrand in our information gain:
$$ w[x; p,q] =  \log \frac{p(x)}{q(x)}. $$
The log ratio of two probability distributions measures by how much you should update your prior log odds between the two distributions being
correct.  The KL divergence is just then the expected weight of evidence if we draw samples from $p(x)$ itself:
$$ I[p;q] = \mathbb{E}_p\left[ \log \frac{p(x)}{q(x)} \right] = \mathbb{E}_p \left[ w[x; p,q] \right]$$&lt;/p&gt;
&lt;p&gt;So, one way to interpret the relative entropy is that if our data was actually coming from the distribution $p$ and we had some other
hypothesis $q$, the $I[p;q]$ measures on average how much we should believe $p$ over $q$ on each observation.  In order to make that
statement more precise, we need a better language to talk about the magnitudes of these quantities.&lt;/p&gt;
&lt;h2&gt;How loud is the Evidence?&lt;/h2&gt;
&lt;p&gt;Our measurement of the amount of information was only unique up to a choice of multiplicative constant.  This is equivalent to
our choice of base for the logarithm.  We can think of this as the &lt;em&gt;units&lt;/em&gt; we use to measure our information.  The traditional choices
would be to use the base-2 logarithm and measure the information in &lt;em&gt;bits&lt;/em&gt;,&lt;sup&gt;&lt;a href="#bit"&gt;8&lt;/a&gt;&lt;/sup&gt;
or to use the more mathematically convenient natural
logarithm and measure the information in &lt;em&gt;nats&lt;/em&gt;.  Another option is to measure the information in
&lt;a href="https://en.wikipedia.org/wiki/Hartley_(unit)"&gt;&lt;em&gt;decibans&lt;/em&gt;&lt;/a&gt; or &lt;em&gt;decibels&lt;/em&gt; or &lt;em&gt;Hartley's&lt;/em&gt;, wherein
we use ten times the base-10 logarithm.&lt;/p&gt;
&lt;aside&gt; &lt;sup id="bit"&gt;8&lt;/sup&gt;
 &lt;i&gt;bit&lt;/i&gt; being short for &lt;i&gt;binary digit&lt;/i&gt;.  
 &lt;i&gt;nat&lt;/i&gt; is then short for &lt;i&gt;natural digit&lt;/i&gt;.
 People sometimes suggest &lt;i&gt;dit&lt;/i&gt; for the base-10 &lt;/i&gt;decimal digit&lt;/i&gt;.
 Turing suggested *ban* as short hand for the amount of evidence deduced about the setting 
 of the Enigma machine using the Banburismus method, itself named after the town of Banbury where
 the team got their large card sheets used in the method.
 For more discussion about the history and etymology of these and related units see section 4.8.1 of
 &lt;a href="https://books.google.com/books/about/Probability_Theory.html?id=tTN4HuUNXjgC&amp;source=kp_book_description"&gt;&lt;i&gt;Probability Theory: The Logic of Science&lt;/i&gt; by E.T. Jaynes&lt;/a&gt;.
&lt;/aside&gt;
$$ I[p;q] = 10 \int \mathrm dx\, p(x) \log_{10} \frac{p(x)}{q(x)}\, \textrm{dB} $$
&lt;p&gt;The nice thing about measuring information in decibans or &lt;a href="https://en.wikipedia.org/wiki/Decibel"&gt;decibels&lt;/a&gt;
is the people already have some familiarity with the unit, such as for measuring the &lt;em&gt;loudness&lt;/em&gt; of sounds.
It's always a comparative measurement, for sound taking $10 \log_{10} \frac{P}{P_0}$ of the power
to some reference or baseline power.  In the same way we could besides just measuring the KL between two distributions,
measure the comparative difference between any two probabilities on the log scale:
$$ 10 \log_{10} \frac{p(x)}{q(x)} \textrm{ dB}. $$&lt;/p&gt;
&lt;p&gt;In particular, we could get some feeling for these quantities by comparing the probability something happens to the
probability it doesn't.  Consider a simple binary outcome and taking $q=1-p$, in this case, the weight of evidence
that the thing happens versus it doesn't upon observing it happen once is:
$$ 10 \log_{10} \frac{p}{1-p} \text{ dB}. $$
This essentially gives us a new scale to measure probabilities on.
Instead of expressing probabilities as a number between 0 and 1,
here we are computing the log &lt;em&gt;odds&lt;/em&gt; of an event happening on the decibel scale.&lt;/p&gt;
&lt;p&gt;Below in Table 1 is a summary of the correspondence between decibans and odds or probabilities, and
in Figure 3 is a large visual representation you can play with.&lt;/p&gt;
&lt;figure&gt;
&lt;center&gt;
&lt;table&gt;
  &lt;thead&gt;&lt;th&gt;db&lt;th&gt;odds&lt;th&gt;~odds&lt;th&gt;probability&lt;th&gt;spinner
  &lt;tr&gt;&lt;td&gt;0&lt;td&gt;1.00&lt;td&gt;1:1&lt;td&gt; 50% 
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-0" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;1&lt;td&gt;1.26&lt;td&gt;5:4&lt;td&gt;56%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-1" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;2&lt;td&gt;1.58&lt;td&gt;π:2&lt;td&gt;61%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-2" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;3&lt;td&gt;2.00&lt;td&gt;2:1&lt;td&gt;67%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-3" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;4&lt;td&gt;2.51&lt;td&gt;5:2&lt;td&gt;71.5%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-4" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;5&lt;td&gt;3.16&lt;td&gt;π:1&lt;td&gt;76%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-5" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;6&lt;td&gt;3.98&lt;td&gt;4:1&lt;td&gt;80%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-6" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;7&lt;td&gt;5.01&lt;td&gt;5:1&lt;td&gt;83%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-7" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;8&lt;td&gt;6.31&lt;td&gt;2π:1&lt;td&gt;86%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-8" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;9&lt;td&gt;7.94&lt;td&gt;8:1&lt;td&gt;89%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-9" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;10&lt;td&gt;10&lt;td&gt;10:1&lt;td&gt;91%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-10" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;11&lt;td&gt;12.6&lt;td&gt;4π:1&lt;td&gt;92.6%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-11" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;12&lt;td&gt;15.8&lt;td&gt;16:1&lt;td&gt;94%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-12" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
  &lt;tr&gt;&lt;td&gt;13&lt;td&gt;20&lt;td&gt;20:1&lt;td&gt;95%
    &lt;td&gt;&lt;svg height="30" width="30" viewBox="0 0 20 20"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-13" stroke="#1f77b4" stroke-width="10" stroke-dasharray="0.942 2.200" /&gt;&lt;/svg&gt;
&lt;/table&gt;
&lt;/center&gt;
  &lt;figcaption&gt;
    Table 1: A table of the correspondence between decibans/decibels and odds or probabilities.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure id="bigspin"&gt;
  &lt;center&gt;
   &lt;svg height="300" width="300" viewBox="-2 -2 25 25"&gt; &lt;circle r="10" cx="10" cy="10" fill="white" stroke="black" stroke-width=0.2 /&gt; &lt;circle r="5" cx="10" cy="10" fill="whitesmoke" id="progress-100" stroke="#1f77b4" stroke-width="9.9" stroke-dasharray="3.141 3.141" /&gt;&lt;/svg&gt;
   &lt;br /&gt;
   &lt;input value=0 type='number' style="width: 4em" id="percent" onchange="updatePercent();"&gt;
   &lt;label for="percent"&gt;dB&lt;/label&gt;
   &lt;br/&gt;
   &lt;input id="slider" style="width: 65%;" type="range" min="-23" step="0.1" max="23" value="0" class="slider" id="slider"
   oninput="updateSlider();" &gt;
  &lt;figcaption&gt;Figure 3: A larger visual representation of decibels as a probability that you can play with. Here the set value
  of decibels measure the weight of evidence between the spinner giving a blue versus a white outcome.&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Another nice property of measuring evidence and probabilities in
decibels is that it seems like 1 dB roughly corresponds the smallest detectable value that people
notice in terms of a change in underlying distribution, being the difference between &lt;i&gt;even chance&lt;/i&gt;
and 5 to 4 odds, &lt;i&gt;moderate probability&lt;/i&gt; or &lt;i&gt;better than even chance&lt;/i&gt;.&lt;/p&gt;
&lt;aside id="quantifying"&gt;&lt;sup&gt;9&lt;/sup&gt; 
  &lt;a href="https://projecteuclid.org/euclid.ss/1177012242"&gt;&lt;i&gt;Quantifying Probabilistic Expressions&lt;/i&gt; by
  Frederick Mosteller and Cleo Youtz&lt;/a&gt;.
&lt;/aside&gt;
Additionally, $10 \textrm{ dB}$ corresponds to 10 to 1 odds, or 91% probability, which people associate
with events being &lt;i&gt;almost certain&lt;/i&gt; or happening &lt;i&gt;almost always&lt;/i&gt;. &lt;sup&gt;&lt;a href="#quantifying"&gt;9&lt;/a&gt;&lt;/sup&gt;.
&lt;p&gt;The traditional statistical threshold for reported results is a &lt;a href="https://en.wikipedia.org/wiki/P-value"&gt;p-value&lt;/a&gt;
of 0.05, which is often &lt;a href="https://en.wikipedia.org/wiki/Misuse_of_p-values"&gt;misinterpreted&lt;/a&gt;
to mean that the probability the null hypothesis is less than
5%.  While this isn't what the p-value measures, if we obtain more than 13 dB of evidence against some
null hypothesis, this does mean that the relative odds that it is correct have decreased by a factor of 20,
taking us below 20 to 1 against if we started with even odds.&lt;/p&gt;
&lt;p&gt;We have the conversions:
$$ 1 \textrm{ nat} = \frac{10}{\log 10} \textrm{ dB} = 4.34 \textrm{ dB} $$
$$ 1 \textrm{ bit} = \frac{10}{\log_2 10} \textrm{ dB} = 3.01 \textrm{ dB} $$&lt;/p&gt;
&lt;h2&gt;Examples and Magnitudes&lt;/h2&gt;
&lt;h3&gt;Double-headed Coin&lt;/h3&gt;
&lt;p&gt;Let's say I have two coins in my pocket, the first is an ordinary unbiased coin, and the second is doubled-headed.
I give you one of them and you start flipping the coin.  You get a heads, then another heads, then another.  How many
heads would you need to see in a row until you're sure you've been given the doubled-headed coin?  Let's
work out the relative entropy between these two distributions.  On the one hand we have $p(H)=1, p(\overline H) =0$,
and the other $q(H) = q(\overline H)= 0.5$.&lt;/p&gt;
&lt;p&gt;$$ I[p;q] = 10 \sum_i p_i \log_{10} \frac{p_i}{q_i} = -10 \log_{10} 2 = 3.01 \text{ dB} $$&lt;/p&gt;
&lt;p&gt;The relative entropy of a sure thing and a coin flip is 3 decibels.  This means that if we want to be more sure than 20 to 1
that we have the doubled-headed coin we'd need to observe 5 heads in a row, giving us 15 dB of evidence.&lt;/p&gt;
&lt;h3&gt;Births&lt;/h3&gt;
&lt;p&gt;Perhaps the first hypothesis test to be resolved with modern statistics was the question of whether more male or female
babies are born.  Using data from 1745 to 1770, Laplace found that in those 26 years, 251,527 boys and 241,945 girls were born.
This gives a fraction of male births of $\sim 51\%$.
Is this just a statistical fluke, or are boys more common than girls at birth?  What Laplace did was to analytically
work out the Bayesian posterior distribution for the probability that a male baby was born using a uniform prior, obtaining
a $\operatorname{Beta}(251528, 241946)$ distribution, for which the probability that the probability a male is born
is less than or equal to $1/2$ is
$$ \int_0^{1/2} \mathrm dx \, \operatorname{Beta}(x; 251528, 241946) \sim 10^{-42}$$
enough for Laplace to declare that he was &lt;em&gt;morally certain&lt;/em&gt; that males
are born more frequently than females.&lt;/p&gt;
&lt;p&gt;Let's work out the weight of evidence in this case, let's say we were comparing two hypotheses, the first
that males are born 51% of the time, and the second that they are born 50% of the time.  With Laplace's data, the
total weight of evidence in this case is:&lt;/p&gt;
&lt;p&gt;$$ 2515270 \log_{10} \frac{0.51}{0.50} + 2419450 \log_{10} \frac{0.49}{0.50} = 404 \text{ dB} $$
a whopping 400 decibels of evidence for males being born 51% of the time rather than 50%.&lt;br /&gt;
At the same time, I'm not sure most people are aware that males are born with a higher proportion and it doesn't
seem to affect most people's lives.  Why is that?  Well, let's evaluate the relative entropy between
a 51% Bernoulli and a 50% Bernoulli:
$$ I = 5.1 \log_{10}\frac{0.51}{0.50} + 4.9 \log_{10} \frac{0.49}{0.50} = 8.7 \times 10^{-4} \text{ dB}. $$
Notice that the relative entropy is quite small.  On average, if the true distribution was 51%, the evidence
we accumulate on each observed birth is less than 8 &lt;em&gt;microbels&lt;/em&gt;.  This means that on average in order to be reasonably
sure that the 51% hypothesis is true, we'd have to observe $\sim \frac{13}{8.7 \times 10^{-4}} \sim 15,000$ births.
This makes clear how with enough data we could both be very sure that males are born with a higher frequency
than females, but at the same time, this could have very little impact on our individual lives.&lt;/p&gt;
&lt;h3&gt;Likelihoods and Learning&lt;/h3&gt;
&lt;p&gt;What we would really like to do is learn a model of some real life distribution.  If the true distribution of data is $p(x)$,
and we have some kind of parametric model $q(x;\theta)$, we would like to set our model parameters $\theta$ so that
we get as close as possible to the true distribution.  In other words, we want to minimize the relative entropy from
the &lt;em&gt;real world&lt;/em&gt; to our &lt;em&gt;model&lt;/em&gt;:
$$\min I[p;q] = \int \mathrm dx\, p(x) \log \frac{p(x)}{q(x;\theta)}. $$
The biggest complication is that we don't actually know what the true distribution of the data is. We can, however, sample data.  Luckily for us, as far as this as an objective for $\theta$ goes, we can treat the entropy of $p(x)$ as
a constant.  This motivates the traditional maximum likelihood objective:
$$ \max \int \mathrm dx \, \log q(x;\theta). $$&lt;/p&gt;
&lt;aside id="gpt3"&gt;&lt;sup&gt;10&lt;/sup&gt;
  For instance, the latest &lt;a href="https://arxiv.org/abs/2005.14165"&gt;GPT-3&lt;/a&gt; model trained by OpenAI,
  was trained on less than half of the training set. (See Table 2.2 in the paper.) 
&lt;/aside&gt;
If we had an infinite dataset, maximum likelihood is the same as minimizing the relative entropy between the real world and 
our model.  Unfortunately, we don't often have infinite datasets.&lt;sup&gt;&lt;a href="#gpt3"&gt;10&lt;/a&gt;&lt;/sup&gt;
On finite datasets, maximum likelihood can still be interpreted as minimizing a KL divergence, but now
the KL divergence between the *empirical distribution* $\hat p(x) = \sum_i \delta(x - x_i) $
and our model $q(x;\theta)$.
&lt;p&gt;Unfortunately, the cross entropy is no longer reparameterization invariant a
point I elaborate in an appendix below, and so is difficult to interpret
directly, but if we take the difference of any two cross entropies, we can
still interpret that as the weight of evidence for one model with regards to
the other.  Because of the lack of reparameterization independence, care must
be taken to ensure that the likelihoods of the two models are evaluated using
the same measure, but provided they are:&lt;/p&gt;
&lt;p&gt;$$ L_1 - L_2 = \mathbb{E}\left[ \log q_1(x) \right] - \mathbb{E}\left[ \log q_2(x) \right] = \mathbb{E}\left[ \log \frac{q_1(x)}{q_2(x)} \right] $$&lt;/p&gt;
&lt;aside id="mnist"&gt;&lt;sup&gt;11&lt;/sup&gt;
  The entirety of which can fit in a &lt;a href="https://twitter.com/alemi/status/1042658244609499137"&gt;tweet&lt;/a&gt;.
&lt;/aside&gt;
Given the size of test sets we have for modern image datasets, this means that very small changes in likelihood can be 
interpreted as large confidences in the superiorities of models.  Take for instance something as simple as binary static MNIST.&lt;sup&gt;&lt;a href="#mnist"&gt;11&lt;/a&gt;&lt;/sup&gt;  Here, with 10,000 test set images, a difference in likelihoods of 0.0013 dB or 0.0004 nats corresponds to 13 dB of evidence for the one model over the second.
&lt;h2&gt;Appendix A: Whither Continuous Entropy&lt;/h2&gt;
&lt;p&gt;The relative entropy really is the proper way to define entropy.  For all
of the things that Shannon got right, he flubbed a bit when he defined the
entropy of a distribution as:
$$ H(P) = -\sum_i p_i \log p_i $$&lt;/p&gt;
&lt;p&gt;Why do I say he flubbed?  Because this notion of entropy doesn't generalize
to continuous distributions.  The continuous analog:
$$ H(P) = -\int \mathrm dx\, p(x) \log p(x) $$
isn't &lt;em&gt;reparameterization independent&lt;/em&gt;.  Consider for instance the distribution
of adult human heights: &lt;sup&gt;&lt;a href="#bimodal"&gt;12&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;figure&gt;
  &lt;center&gt;
  &lt;img src="figures/adult_heights.svg"
    alt="Distribution of adult heights."&gt;
  &lt;figcaption&gt;Figure 1. Distribution of adult heights. &lt;sup&gt;&lt;a href="#ourworld"&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;aside&gt; &lt;sup id="bimodal"&gt;12&lt;/sup&gt;
  Note that you may have heard that 
  &lt;a href="https://www.johndcook.com/blog/2008/07/20/why-heights-are-normally-distributed/"&gt;heights are normally distributed&lt;/a&gt;.
  Adult male (or female) heights are normally distributed, but differ in their means and variances, making the 
  &lt;a href="https://www.johndcook.com/blog/2008/11/25/distribution-of-adult-heights/"&gt;distribution of adult heights a mixture distribution&lt;/a&gt;.
&lt;/aside&gt;
&lt;aside&gt; &lt;sup id="ourworld"&gt;13&lt;/sup&gt;
  Data taken from 
  &lt;a href="https://ourworldindata.org/human-height"&gt;ourworldindata.org&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;If you measure the continuous entropy of this distribution measured
in centimeters you get 5.4 bits.  If you instead measure the entropy
of the same distribution in feet you get 0.43 bits.  If you instead
were to measure heights in meters it would be -1.3 bits! &lt;sup&gt;&lt;a href="#negative"&gt;14&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;aside&gt; &lt;sup id="negative"&gt;14&lt;/sup&gt;
  It seems strange to have a negative entropy, but in this case, it is basically
  reflecting the fact that in terms of meters, the human height distribution doesn't
  span a whole meter in breadth, so it actually takes fewer *relative* bits
  to specify a human height in meters than it would take to specify any
  quantity in meters, because its uncertainty is less than a whole meter.
&lt;/aside&gt;
&lt;h2&gt;Appendix B: Coding Interpretation&lt;/h2&gt;
&lt;p&gt;The traditional interpretation offered for the KL is from the coding
perspective.
Imagine we have a simple 4-letter
alphabet that we want to communicate over the wire.
If the four letters occurred with different probabilities:
$p(A)=1/2, p(B)=1/4, p(C)=p(D)=1/8$, with an optimally designed &lt;a
href="https://en.wikipedia.org/wiki/Huffman_coding"&gt;Huffman Code&lt;/a&gt; we could
encode our letters with a variable length code: $A:0, B:10, C:110, D:111$, and
on average we'd only be spending $1/2 + 2/4 + 3/8 + 3/8 = 7/4$ bits per letter.&lt;/p&gt;
&lt;figure&gt;
  &lt;center&gt;
  &lt;table&gt;
    &lt;thead&gt;&lt;th&gt;&lt;th&gt;A&lt;th&gt;B&lt;th&gt;C&lt;th&gt;D
    &lt;tr&gt;&lt;td&gt;$p$&lt;td&gt;1/2&lt;td&gt;1/4&lt;td&gt;1/8&lt;td&gt;1/8
    &lt;tr&gt;&lt;td&gt;p-code&lt;td&gt;0&lt;td&gt;10&lt;td&gt;110&lt;td&gt;111
    &lt;tr&gt;&lt;td&gt;$q$&lt;td&gt;1/4&lt;td&gt;1/4&lt;td&gt;1/4&lt;td&gt;1/4
    &lt;tr&gt;&lt;td&gt;q-code&lt;td&gt;00&lt;td&gt;01&lt;td&gt;10&lt;td&gt;11
  &lt;/table&gt;
  &lt;figcaption&gt;
    Table 2: A simple example of two different distributions over a 4 letter alphabet.
  &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Imagine however we didn't know what the true distribution of letters was and instead
designed an optimal code using a different distribution $q$.  If we believed
each of the 4 letters were equally likely $(q(A)=q(B)=q(C)=q(D)=1/4)$, the optimal way to
encode messages would just assign a two bit code to each letter $(A : 00, B:01, C:10,
D:11)$.  If we used this suboptimal code to send messages that were actually distributed
as $p$ it would cost $2/2 + 2/4 + 2/8 + 2/8 = 2$ bits per letter.  Our incorrect
belief leads to a $2 - 7/4 = 1/4$ of a bit inefficiency.  For these two distributions,
it shouldn't come as a surprise that the information gain is precisely 1/4 bits:
$$ I[p;q] = \sum_i p_i \log_2 \frac{p_i}{q_i} = 1/4 \textrm{ bits}. $$&lt;/p&gt;
&lt;p&gt;For an optimally designed code, the code lengths go as $-\log p(x)$ for any symbol $x$.
Our information gain can be interpreted as a difference in expected code lengths under $p$:
$$ I[p;q] = \mathbb{E}_p[ -\log q ] - \mathbb{E}_p[-\log p ]. $$
The information gain $I[p;q]$ measures the &lt;em&gt;excess encoding cost&lt;/em&gt; for trying to encode messages
from $p$ using a code designed for $q$.&lt;/p&gt;
&lt;script type='text/javascript'&gt;
const SEGMENTS = 5;
const RADIUS = 5;
const CIRCUMFERENCE = 2 * Math.PI * RADIUS;

function fraction(i, db) {
  const progress = document.getElementById('progress-' + i);
  let odds = Math.pow(10.0, db / 10.0);
  let p = odds / (1+odds);
  let fill = CIRCUMFERENCE / SEGMENTS * p;
  let space = CIRCUMFERENCE / SEGMENTS * (1-p);
  let val = fill + " " + space;
  progress.style.strokeDasharray = val;
}

for (let i = 0; i &lt;= 13; i++) {
  fraction(i,i);
}

function updateSlider() {
  let value = document.getElementById("slider").value;
  fraction(100, value);
  document.getElementById("percent").value = value;
}

function updatePercent() {
  let value = document.getElementById("percent").value;
  document.getElementById("slider").value = value;
  fraction(100, value);
}
&lt;/script&gt;
</content:encoded><guid isPermaLink="true">https://blog.alexalemi.com/kl.html</guid><category domain="https://alexalemi.com/posts/">posts</category><pubDate>Fri, 07 Aug 2020 00:00:00 -0400</pubDate></item><item><title>Blogging</title><link>https://blog.alexalemi.com/blogging.html</link><description>I miss blogging and reading other researchers blogs.</description><content:encoded>&lt;p&gt;We all stand on the shoulders of giants.&lt;/p&gt;
&lt;p&gt;The thing that separates humans from other animals is the degree to which we can share knowledge with one another.  As such, I feel as though we have some duty to our fellow humans to share the knowledge we've accumulated. More than ever before, everyone has at their disposal the tools required to share their knowledge with anyone essentially anywhere else on earth.&lt;/p&gt;
&lt;p&gt;I feel guilty for not participating myself.&lt;/p&gt;
&lt;p&gt;Sure I write scientific papers, but if we're being honest, most scientific papers have rather small audiences. If we're being even more honest, unfortunately, it seems as though most scientific papers these days are not really meant to be read. It doesn't feel as though they go out of their way to make themselves understood.  I'm sure there is some selection bias but I feel as though when I read older papers they feel a lot more like a conversation with the author. I feel as though the best papers feel as though they are a chat with the author(s) and they are essentially helping you stand on their shoulders. They've spent a good amount of time and energy thinking about some particular thing and are attempting to share that thought with you so that you don't have to go through the same arduous process.&lt;/p&gt;
&lt;p&gt;I feel as though this is a perfect opportunity to bring back blogging and in particular academic blogs. For some reason, it becomes clear when you're writing a blog post that it's meant to be read. Many academic blogs take on a much more conversational tone and focus a lot more on explaining themselves and the ideas they are covering in simple to understand terms. Granted, a lot of the best information these days comes in the form of &lt;a href="https://www.youtube.com/3blue1brown"&gt;videos&lt;/a&gt;, but not everyone has the skills, time, or energy to produce such amazing video content.  The barrier to entry is a lot lower for blogging.&lt;/p&gt;
&lt;p&gt;Finally, it's cliche but we all know that the best way to learn is through teaching. Taking the time to try to explain a new concept you're struggling with can really help solidify it in your own mind.  I've found that many of the things I've learned best were things I worked through for previous blog posts of mine.&lt;/p&gt;
&lt;p&gt;I want to start blogging again. I've set up this small site to do that. It's remarkable how easy it is these days.  This site is being hosted on &lt;a href="https://pages.github.com/"&gt;GitHub Pages&lt;/a&gt; which gives free space for anyone to start hosting their own static content. Using systems like &lt;a href="https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll"&gt;Jekyll&lt;/a&gt; the actual practice of making the website can be quite simple these days.  In my case I wrote some &lt;a href="https://github.com/alexalemi/alexalemi.github.io/tree/master/src"&gt;custom python scripts&lt;/a&gt; to convert my &lt;a href="https://daringfireball.net/projects/markdown/"&gt;Markdown&lt;/a&gt; posts to html. This blog itself lives in its own &lt;a href="https://github.com/alexalemi/blog.alexalemi.com/"&gt;github repo&lt;/a&gt;.  &lt;a href="https://www.mathjax.org/"&gt;MathJax&lt;/a&gt; allows for beautiful $\LaTeX$ math rendering.
For discussions or comments, I'll try out &lt;a href="https://giscus.app/"&gt;giscus&lt;/a&gt; which is a nifty system enabling comments through &lt;a href="https://github.com/alexalemi/blog.alexalemi.com/discussions"&gt;github discussions&lt;/a&gt;. People can also reach out on &lt;a href="https://twitter.com/alemi"&gt;twitter&lt;/a&gt;. I also think it's important to support &lt;a href="https://atthis.link/blog/2021/rss.html"&gt;rss&lt;/a&gt; which I've done here.&lt;/p&gt;
</content:encoded><guid isPermaLink="true">https://blog.alexalemi.com/blogging.html</guid><category domain="https://alexalemi.com/posts/">posts</category><pubDate>Mon, 15 Nov 2021 00:00:00 -0500</pubDate></item><item><title>A Simple Demonstrations of Benford's Law</title><link>https://blog.alexalemi.com/benford.html</link><description>Benford's law is a natural consequence of scale invariance, this is relatively easy to see in the face of a circular sliderule.</description><content:encoded>&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Benford%27s_law"&gt;Benford's Law&lt;/a&gt; is the
observation that in a list of &amp;quot;naturally occuring numbers&amp;quot;,
the plurality of numbers will begin with a 1.
This often catches people by surprise, but if you go and pull random numbers from a book or newspaper you can expect the leading digits to follow the following distribution:&lt;/p&gt;
&lt;figure id="#benford" class="right"&gt;
  &lt;center&gt;
  &lt;img width="95%" src="figures/benford-distribution.svg"
    alt="Benford's distribution."&gt;
  &lt;figcaption&gt;
  Figure 1. The distribution of leading digits in naturall occuring numbers.
  &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;Nearly a third of all naturally occuring numbers begin with a 1.  Why?
Perhaps the simplest way to see this is to realize that if there is going to be some kind of distribution for naturally occuring numbers, that distribution ought to be reparameterization independent.  It shouldn't matter whether we use English or metric units.  If there is going to be a universal leading digit distribution, its gotta be invariant to a change of units.  Changing units is accomplished via multiplication, so whatever the universal distribution of leading digits is, it needs to be invariant to multiplication.&lt;/p&gt;
&lt;h2&gt;See it&lt;/h2&gt;
&lt;p&gt;Perhaps the easiest way to &lt;em&gt;see&lt;/em&gt; Benford's law is to look at a Circular &lt;a href="https://en.wikipedia.org/wiki/Slide_rule"&gt;slide rule&lt;/a&gt;:&lt;sup&gt;&lt;a href="#kl-1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;figure id="#circular-slide-rule" class="right"&gt;
  &lt;center&gt;
  &lt;img width="95%" src="figures/circular-slide-rule.jpg"
    alt="Circular slide rule."&gt;
  &lt;figcaption&gt;
  Figure 2. An old soviet circular slide rule.  The inner dial is the main dial.  Notice that the digits follow Benford's law.
  &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;aside&gt; &lt;sup id="kl-1"&gt;1&lt;/sup&gt;
This is a picture of one of my &lt;a href="https://collection.maas.museum/object/383283"&gt;Soviet KL-1 circular slide rules&lt;/a&gt;, which I'll release in the public domain under &lt;a href="https://creativecommons.org/publicdomain/zero/1.0/deed.en"&gt;CC0&lt;/a&gt;. You can see a demo of how to use these slide rules &lt;a href="https://www.youtube.com/watch?v=Kuzdjy3HpWg"&gt;here&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;If you look at the inner dial, you'll notice that the digits are spaced just like our Benford's law distribution was in Figure 1.  This is not an accident.  Slide rules work by physically manifesting multiplication as a sort of addition.  On this circular slide rule, if you add the angle a number appears at on the inner dial to the angle some other number appears at, the resulting angle will point to their product.  Since slide rules only track the significand,&lt;sup&gt;&lt;a href="#significand"&gt;2&lt;/a&gt;&lt;/sup&gt; circular slide rules cleverly wrap around.&lt;sup&gt;&lt;a href="#sliderule"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;aside&gt; &lt;sup id="significand"&gt;2&lt;/sup&gt;
The &lt;a href="https://en.wikipedia.org/wiki/Significand"&gt;&lt;em&gt;significand&lt;/em&gt;&lt;/a&gt; is the front part of a number when written in scientific notation, that is its the significant digits expressed as a number between 1 and 10.  For example, the significand of 2342.1231 is 2.3421231, the significand of 0.00234 is 2.34.  You simply move the decimal point to be just to the right of the first non-zero digit.
As a formula its $\frac{|x|}{10^{\lfloor \log_{10} |x| \rfloor}}$.
&lt;/aside&gt;
&lt;aside&gt; &lt;sup id="#sliderule"&gt;3&lt;/sup&gt;
If you want to play with a circular sliderule yourself, you can try &lt;a href="https://alexalemi.com/random/sliderule.html"&gt;this one I built&lt;/a&gt;.
&lt;/aside&gt;
&lt;p&gt;Whatever the universal leading digit distribution is, or more specifically whatever the universal distribution of significands is, provided one exists, it would have to be invariant to any multiplication.  It would have to be invariant to the addition of any random angle on the circular slide rule, i.e. it would have to be circularly symmetric, i.e. it would have to uniform on the slide rule dial.&lt;/p&gt;
&lt;p&gt;This thought process is enough to give us the distribution in Benford's law.  The digits on the circular slide rule are located so that,
$$ \theta(x) = 2 \pi \log_{10} x, $$
for the numbers from 1 to 10.  This ensures that 1 is at $\theta=0$ and $10$ is at $\theta = 2\pi$.  It also ensures that if we try to locate the angle of a product of numbers $x$ and $y$, we can do so by simply adding their angles:&lt;sup&gt;&lt;a href="#theory"&gt;4&lt;/a&gt;&lt;/sup&gt;
$$ \theta(x y) = 2\pi \log_{10}( x y ) = 2 \pi \log_{10} x + 2 \pi \log_{10} y = \theta(x) + \theta(y). $$&lt;/p&gt;
&lt;aside&gt; &lt;sup id="#theory"&gt;4&lt;/sup&gt;
This is all there really is to how slide rules work, they turn multiplication into addition.  Addition is something easy to do with linear or circular rules: you &lt;em&gt;add&lt;/em&gt; distances or angles by simply &lt;em&gt;sticking them next to each other&lt;/em&gt;. $\log (xy) = \log x + \log y$.  We have the benefit of hindsight and knowledge of logarithms, though this idea, the desire to have a function that would enable turning multiplication into a simple sort of sticking together was was lead &lt;a href="https://en.wikipedia.org/wiki/John_Napier"&gt;John Napier&lt;/a&gt; to the logarithm function in the first place! If you want to learn more about slide rules, I recommend &lt;a href="https://www.sliderulemuseum.com/Manuals/M220_AnEasyIntroductionToTheSlideRule_IsaacAsimov_1965.pdf"&gt;Asimov's book&lt;/a&gt;.
&lt;/aside&gt;
Knowing how the digits are arranged, we can easily determine the fraction of the circle allotted to each one:
$$ f(d) = \log_{10}(d+1) - \log_{10}(d) = \log_{10}\left( 1 - \frac{1}{d} \right). $$
This is the formula you'll see &lt;a href="https://en.wikipedia.org/wiki/Benford\%27s_law"&gt;elsewhere&lt;/a&gt;.  Most of the discussion surrounding Benford's law focusses on the first digit alone, but our visual argument also suggests that we can easily determine the distribution for significands themselves, not just the first digit.  For instance, looking at the sliderule, we can see that its nearly as likely that a number should have "10" as its first &lt;em&gt;two&lt;/em&gt; digits&lt;sup&gt;&lt;a href="#1.1"&gt;5&lt;/a&gt;&lt;/sup&gt; as it is that we'd find a naturally occuring number beginning with a 9.
&lt;aside&gt; &lt;sup id="#1.1"&gt;5&lt;/sup&gt;
Specifically that the significand should be between 1.0 and 1.1.
&lt;/aside&gt;
&lt;p&gt;We've already said that with a random multiplication being like a random spin of the circular slide rule pointer, the universal distribution of significands should just be uniform on the slide rule.  Performing a change of basis, if we know the distribution $p(\theta)$ of angles along the circle is uniform, we can work out the distribution $p(x)$ of significands by requiring we conserve all of the probability mass:
$$ p(\theta) \, d\theta &amp;amp;= p(x) \, dx $$
combined with what we already know as the relationship between our significands and their angles: $\theta = 2\pi \log_{10} x$. This allows us to transform the uniform distribution of angles $p(\theta) = \frac{1}{2\pi}$ into:
$$ p(x) = \frac{1}{x \log 10 }. $$
We've recovered a nice &lt;a href="https://en.wikipedia.org/wiki/Power_law"&gt;power law&lt;/a&gt; or &amp;quot;scale-free&amp;quot; distribution for the significands, something we could have guessed or worked out from our requirement that the distribution be invariant to scale.&lt;/p&gt;
&lt;p&gt;We may have just gone around in a circle,&lt;sup&gt;&lt;a href="#pun"&gt;6&lt;/a&gt;&lt;/sup&gt; but I hope you agree that there is something very visceral about seeing Benford's law play out on the face of the circular sliderule.&lt;/p&gt;
&lt;aside&gt; &lt;sup id="#pun"&gt;6&lt;/sup&gt;
Pun very much intended.
&lt;/aside&gt;
</content:encoded><guid isPermaLink="true">https://blog.alexalemi.com/benford.html</guid><category domain="https://alexalemi.com/posts/">posts</category><pubDate>Tue, 03 May 2022 00:00:00 -0400</pubDate></item><item><title>Knots</title><link>https://blog.alexalemi.com/knots.html</link><description>A short list of useful knots to know.</description><content:encoded>&lt;p&gt;Years ago I decided that I didn't know how to tie any knots and needed to
remedy the situation.  Unfortunately, if you start to look into knots you
discover there is a &lt;a href="https://en.wikipedia.org/wiki/List_of_knots"&gt;whole slew of them&lt;/a&gt;.
It's easy to dive deep and try out
dozens of different knots and then be left not remembering how to tie any of
them.  So, I decided I was going to try to pick the best knot for each of a few
different scenarios and then focus on learning and retaining those knots well.&lt;/p&gt;
&lt;p&gt;I can say that having done this nearly a decade ago, it's paid off.  It's not the
kind of thing that comes up very often, but when it does, knowing what to do
with a rope has proven a very useful skill.  So, which knots do I recommend?&lt;/p&gt;
&lt;p&gt;Most sources of this type tend to focus on specific jobs or activities, the
best boating knots, scouting knots, climbing knots, etc.  I don't do any of
those things.  Instead, I want to know how to not embarrass myself if I find
myself needing to accomplish some kind of task with a rope.  So, for each of
the common use cases for a rope, I've tried to identify the &amp;quot;best&amp;quot; knot to use.
Best, in this case, means some combination of ease of tying, ease of remembering,
and strength and ability of the knot. A lot of knots will have &amp;quot;false friends&amp;quot;
of a sort, very closely related knots that are often much worse in their
characteristics.  I tried to pay special attention here, as I don't very often
use knots so I need to ensure the ones I focus on learning are robust in the
sense that even if I haven't tied it for a few years I'm unlikely to make a
grave mistake.&lt;/p&gt;
&lt;h2&gt;Tying off&lt;/h2&gt;
&lt;p&gt;If I'm being honest, the most common knot I tie, nearly every day is the
&lt;a href="https://www.animatedknots.com/square-knot"&gt;Square knot&lt;/a&gt;, also known as the
&lt;a href="https://en.wikipedia.org/wiki/Reef_knot"&gt;reef knot&lt;/a&gt;.  This is because its the
knot I use to tie my shoes, or tie off garbage bags, that sort of thing.&lt;/p&gt;
&lt;figure class="right"&gt;
  &lt;center&gt;
  &lt;img width="70%" src="figures/knots/reef.jpg" /&gt;
  &lt;figcaption&gt;
    &lt;small&gt;
    &lt;a title="beautiful [but deadly] square knot" href="https://flickr.com/photos/wwworks/5623339500"&gt;beautiful [but deadly] square knot&lt;/a&gt; flickr photo by &lt;a href="https://flickr.com/people/wwworks"&gt;woodleywonderworks&lt;/a&gt; shared under a &lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;Creative Commons (BY) license&lt;/a&gt;
    &lt;/small&gt;
  &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;The knot is very common and nearly everyone knows it, but one thing I've
discovered is that a good fraction of people tie the knot incorrectly!
By far, the most useful thing I learned all those years ago when I decided to
dive into knots was that I had been tying my shoes wrong &lt;em&gt;my entire life&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To tie a proper reef knot, you have to alternate which rope goes on top.  The
mnemonic is &lt;em&gt;right over left, then left over right&lt;/em&gt;, if you instead do &lt;em&gt;right
over right then right over right&lt;/em&gt; as I did most of my life, you end up with the
far worse knot, the &lt;a href="https://en.wikipedia.org/wiki/Granny_knot"&gt;Granny Knot&lt;/a&gt;,
instead.&lt;/p&gt;
&lt;p&gt;Throughout my childhood and young adult life, it always seemed as though my
shoelaces would come untied, I often had to revert to double knotting them to
try to get them to last the whole day.  The problem the entire time was that I
was tying the wrong knot.  I've since caught many of my friends and family
having the exact same problem.  If you spend a few minutes tying both a proper
square knot and a granny knot you can immediately learn to tell the difference.
The telltale sign when looking at someone's shoes is to check to see if the
loops are &lt;em&gt;crooked&lt;/em&gt;.  Proper shoelace loops should lie down the sides of the
shoe.  If a granny knot is tied, they tend to rotate and lie parallel to the
shoe but perpendicular to the laces.  For more info, see &lt;a href="https://www.fieggen.com/shoelace/grannyknot.htm"&gt;Ian Fieggen's
site&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now, as I said, the square knot is by far the most common knot I tie, which is
a bit unfortunate because the knot itself is pretty poor. It should &lt;em&gt;not&lt;/em&gt; be used
in any sort of critical situation.  Aside from tying off garbage bags and
shoelaces, it shouldn't be used at all.  &lt;em&gt;It should never be used to join two
ropes&lt;/em&gt;. Ashley&lt;sup&gt;&lt;a href="#ashley"&gt;1&lt;/a&gt;&lt;/sup&gt; claimed that reef knots have
caused more deaths than all other knots combined.  Given their familiarity, I
imagine many people use them to join ropes not realizing their tendency to slip
until it's too late.  Learn how to tie a square knot properly but also learn
alternatives to use for basically all use cases.&lt;/p&gt;
&lt;aside&gt; &lt;sup id="ashley"&gt;1&lt;/sup&gt; 
  Ashley of &lt;a href="https://en.wikipedia.org/wiki/The_Ashley_Book_of_Knots"&gt;&lt;i&gt;The Ashley Book of Knots&lt;/i&gt;&lt;/a&gt;,
   a truly remarkable book.  Published in 1944 it remains the goto reference on
   knots. Ashley spent 11 years compiling over 3800 numbered entries and 7000
   of his own illustrations.  It's an extremely impressive bit of scholarship.
&lt;/aside&gt;
&lt;h3&gt;Shoelaces&lt;/h3&gt;
&lt;p&gt;Speaking of Ian Fieggen, I actually use his
&lt;a href="https://www.animatedknots.com/shoelace-bow-knot-fieggen-method"&gt;quick method&lt;/a&gt; to try
my shoes these days. It only takes a few minutes to learn and then you can use
it for the rest of your life, I recommend checking it out.&lt;/p&gt;
&lt;figure&gt;
&lt;center&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/_O-xaJrao1w" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;figurecaption&gt;A very fast method of tying your shoes.
&lt;/figurecaption&gt;
&lt;/figure&gt;
&lt;h2&gt;Loop at end of the rope&lt;/h2&gt;
&lt;p&gt;The next most common use case I find myself in is wanting to tie a loop at the
end of a rope.  For this I think the
&lt;a href="https://www.animatedknots.com/bowline-knot"&gt;Bowline&lt;/a&gt; is the goto answer.&lt;/p&gt;
&lt;figure class="right"&gt;
  &lt;center&gt;
  &lt;img width="50%" src="figures/knots/bowline.jpg" /&gt;
  &lt;figcaption&gt;
    &lt;small&gt;
      "Bowline knot", 
      by &lt;a href="https://en.wikipedia.org/wiki/Bowline#/media/File:Palstek_innen.jpg"&gt;Markus Bärlocher&lt;/a&gt;,
      in the public domain.
    &lt;/small&gt;
  &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;The Bowline is often described as the &amp;quot;king of knots&amp;quot;.  It's a nice knot to
know.  The loop formed doesn't collapse, so can be used to make a handle, to
tie a rope around a hole, pole, or other objects.  If you make a Bowline in the
end and then pass the rest of the rope through you can make a type of lasso or
slipknot. Even after being used the knot is usually easy to untie.&lt;/p&gt;
&lt;p&gt;Tying the knot is simple and easy to remember, especially with the common
mnemonic: &lt;em&gt;&amp;quot;The rabbit comes up the hole, runs around the tree then back down
the hole.&amp;quot;&lt;/em&gt;  One thing to pay special attention to is to ensure that the free
end of the rope is left on the inside of the loop.&lt;/p&gt;
&lt;h2&gt;Loop in the middle of the rope&lt;/h2&gt;
&lt;p&gt;Next, if you need a loop in a the middle of a rope and don't have access to the ends, use the
&lt;a href="https://www.animatedknots.com/alpine-butterfly-loop-knot"&gt;Alpine Butterfly&lt;/a&gt;.  This is a great midline loop knot, it doesn't
collapse, can be loaded from any direction or the loop itself.  It is easy to untie after being loaded.&lt;/p&gt;
&lt;figure&gt;
  &lt;center&gt;
  &lt;img width="50%" src="figures/knots/alpine-butterfly.jpg" /&gt;
  &lt;figcaption&gt;
    &lt;small&gt;
      "Alpine butterfly knot", 
      by &lt;a href="https://commons.wikimedia.org/wiki/File:Alpine_butterfly_knot_WPK.jpg"&gt;WikipedianYknOK&lt;/a&gt;,
      licensed under
      &lt;a href="https://creativecommons.org/licenses/by-sa/3.0"&gt;CC BY-SA 3.0&lt;/a&gt;
    &lt;/small&gt;
  &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;It's also very easy to tie if you know the hand wrap method on &lt;a href="https://www.animatedknots.com/alpine-butterfly-loop-knot"&gt;Grog's site&lt;/a&gt;.&lt;br /&gt;
Using this method, its very difficult to mess up the knot.&lt;/p&gt;
&lt;h2&gt;Bend, joining two ropes&lt;/h2&gt;
&lt;p&gt;To &lt;em&gt;bend&lt;/em&gt; or join two ropes together, we don't actually need a new knot, we can reuse the
&lt;a href="https://www.animatedknots.com/alpine-butterfly-bend-knot"&gt;Alpine Butterfly&lt;/a&gt;.&lt;/p&gt;
&lt;figure&gt;
  &lt;center&gt;
  &lt;img width="50%" src="figures/knots/alpine-butterfly-bend.jpg" /&gt;
  &lt;figcaption&gt;
    &lt;small&gt;
      "AlpineButterflyBend", 
      by &lt;a href="https://commons.wikimedia.org/wiki/File:AlpineButterflyBend.JPG"&gt;Cobanyastigi&lt;/a&gt;,
      in the public domain.
    &lt;/small&gt;
  &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;p&gt;To tie this, use the same hand wrapping method but start with the first rope
which you take up to the top of your hand and pinch between your middle
fingers, then add the second rope to the same spot and continue the wrap.  Pull
both ends down and under the two wraps and you've got an alpine butterfly but
with the two ends of the ropes playing the role of the loop.&lt;/p&gt;
&lt;p&gt;We get another use case without having to remember a new knot.&lt;/p&gt;
&lt;h2&gt;Hitch, attaching a rope to an object&lt;/h2&gt;
&lt;p&gt;I have to admit, while last time I looked into knots I managed to learn all the
rest of the knots you see listed here, but I didn't manage to remember the hitch I
had picked.&lt;br /&gt;
This is unfortunate because I've found myself needing to tie a rope to a pole
or tree at times.  One option is to simply tie a Bowline loop around the object
in question, which I've used in the past, but I've felt bad for not having a proper hitch in my
arsenal.
Since I forgot the hitch I picked last time around, I clearly made a poor choice,
so when preparing this blog post, I started over.&lt;/p&gt;
&lt;p&gt;After looking around again,
I've decided that going forward, my chosen hitch will be the
&lt;a href="https://www.youtube.com/watch?v=nMJkfMPc6J8"&gt;Backhand Hitch&lt;/a&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/nMJkfMPc6J8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;p&gt;The backhand hitch is a
&lt;a href="https://www.animatedknots.com/munter-mule-combination-hitch-knot"&gt;Munter hitch&lt;/a&gt; with
&lt;a href="https://www.animatedknots.com/two-half-hitches-knot"&gt;Two Half Hitches&lt;/a&gt; to
finish it off. It appears to be gaining in popularity, with
&lt;a href="https://youtu.be/gv1e4kkgbrI"&gt;many&lt;/a&gt; &lt;a href="https://youtu.be/M4ktpEtz_5c"&gt;extolling&lt;/a&gt;
its &lt;a href="https://youtu.be/IQDEns_AiP0"&gt;benefits&lt;/a&gt;.  It's a secure knot that is not only
easy to untie but can hold a load while being untied due to the Munter hitch
at the base.&lt;/p&gt;
&lt;p&gt;As I said I don't have a whole lot of personal experience with this knot, but I'll try
to retain it going forward.&lt;/p&gt;
&lt;h3&gt;Trucker's Hitch&lt;/h3&gt;
&lt;p&gt;Another reason I like the Backhand hitch is that it sets us up well to make use of another kind of popular hitch,
the &lt;a href="https://youtu.be/IQDEns_AiP0"&gt;Trucker's Hitch&lt;/a&gt;&lt;sup&gt;&lt;a href="#ylvis"&gt;2&lt;/a&gt;&lt;/sup&gt;.  I think of the Trucker's hitch
less as a particular knot as more as a sort of principle for how to tie down loads.  A trucker's hitch involves tying a mid-line
loop in a rope which you then use a pulley to get three-to-one leverage as you try to pull the line tight.  After that, you simply tie
it off when it's appropriately tight. The three-to-one leverage you get really helps tighten the rope.&lt;/p&gt;
&lt;aside&gt; &lt;sup id="ylvis"&gt;2&lt;/sup&gt; 
  For a not-so-informative but definitely entertaining overview of the Trucker's Hitch, 
  see the music video by &lt;a href="https://youtu.be/TUHgGK-tImY"&gt;Ylvis&lt;/a&gt; (of &lt;a href="https://youtu.be/jofNR_WkoCE"&gt;What Does the Fox Say?&lt;/a&gt; fame). 
  &lt;i&gt;Also note that at &lt;a href="https://youtu.be/TUHgGK-tImY?t=99"&gt;3:23 in the video&lt;/a&gt;,
  you can tell that they tied their double shoelace bow in the incorrect Granny Knot orientation&lt;/i&gt;.
&lt;/aside&gt;
&lt;p&gt;For our midline loop, we can use the Alpine butterfly from above.  After pulling tight, we need to secure the rope,
and for that, we can finish the Trucker's hitch with the same &lt;a href="https://www.animatedknots.com/two-half-hitches-knot"&gt;Two Half Hitch&lt;/a&gt;
structure we use to finish the Munter Hitch in the Backhand Hitch, here around both of the ropes at the same time.&lt;/p&gt;
&lt;p&gt;Even though it's composed of the other pieces we've discussed so far, it's worth practicing the Trucker's hitch altogether, and with some regularity.  It's
the kind of thing that if you can recall immediately can be a big help, but if you have to look it up, it's not really worth it.&lt;/p&gt;
&lt;h2&gt;Necktie&lt;/h2&gt;
&lt;p&gt;I don't often wear a tie, but when I do I use the &lt;a href="https://www.animatedknots.com/tie-a-pratt-necktie-knot"&gt;Pratt knot&lt;/a&gt;.
A modern, relatively slim, nicely symmetrical tie knot.  It has served me well, and I like that it starts with the tie reversed.&lt;/p&gt;
&lt;figure&gt;
  &lt;center&gt;
  &lt;img width="50%" src="figures/knots/pratt.png" /&gt;
  &lt;figcaption&gt;
    &lt;small&gt;
      "Blue Pratt Knot", 
      by &lt;a href="https://commons.wikimedia.org/wiki/File:Pratt_Knot.png"&gt;Kris Alekseych Karlov&lt;/a&gt;,
      in the public domain.
    &lt;/small&gt;
  &lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;
&lt;h2&gt;Binding&lt;/h2&gt;
&lt;p&gt;If you want to very securely close off a bag or sack, so well that you'll have to cut the rope
to get it open again, look no further than the &lt;a href="https://www.animatedknots.com/constrictor-knot-twisting-method"&gt;Constrictor Knot&lt;/a&gt;.
It can also be tied &lt;a href="https://www.animatedknots.com/constrictor-knot-rope-end-method"&gt;in the end of a rope&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Stopper Knot&lt;/h2&gt;
&lt;p&gt;Finally, though I don't think it's the most important use case, a surprisingly
common one is needing to put a stopper knot in a rope, just something to keep
the rope from slipping through a hole.  Nearly everyone knows the [Overhand &lt;a href="https://www.animatedknots.com/overhand-knot"&gt;knot&lt;/a&gt;, however, I think fewer people know that there are superior alternatives.  The overhand knot tends to bind, being difficult to untie if
pulled taught, and isn't particularly big a stopping knot, to begin with.&lt;/p&gt;
&lt;p&gt;If the desire is to have a large stopper knot, the &lt;a href="https://www.animatedknots.com/double-overhand-stopper-knot"&gt;Double overhand&lt;/a&gt;
is much better than the ordinary overhand knot and very easy to tie, simply add an additional turn while tying the overhand knot.
When pulled tight it forms a neatly symmetrical stopper, but tends to bind tight, perhaps tighter than the overhand knot.&lt;/p&gt;
&lt;p&gt;If you want a stopper knot that can be untied, the &lt;a href="https://www.animatedknots.com/figure-8-knot"&gt;Figure 8&lt;/a&gt;
knot works well, and could also be described as adding an additional to the overhand knot, but this time on the outside of the knot.&lt;/p&gt;
&lt;h2&gt;Try them out&lt;/h2&gt;
&lt;p&gt;Get yourself a piece of rope, and a free half-hour or so, and try out some of these knots.
If you focus on a short list of a handful of knots,
one for each situation you might need to use a rope for, I think you'll find it time well spent.&lt;/p&gt;
</content:encoded><guid isPermaLink="true">https://blog.alexalemi.com/knots.html</guid><category domain="https://alexalemi.com/posts/">posts</category><pubDate>Mon, 13 Jun 2022 00:00:00 -0400</pubDate></item></channel></rss>